{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Step4-inference.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"j1was3nBjl3A","colab_type":"text"},"cell_type":"markdown","source":["**inference: Using stage2 features from step1 and wts from step2 and 3, do inference**\n","\n","sub_A: using 2000 development + 2000 test + 400 validation to train (all_train)\n","\n","sub_B: using 2000 test + 454 validation to train (not all_train)\n","\n"]},{"metadata":{"id":"uN5qiNQMz-Po","colab_type":"text"},"cell_type":"markdown","source":["sub_A is the one I described in Disscussion post. sub_B differs from sub_A in (1) it's trained on only 2000 test and 454 validation data; (2) it didn't use the linguistic features. This was my backup submission. I expected it to perform worse than sub_A"]},{"metadata":{"id":"eTwZd6T_heHd","colab_type":"text"},"cell_type":"markdown","source":["**Instructions:**\n","\n","sub_A: Turn `all_train` to `True` in next cell and run everything except sub_B sections\n","\n","sub_B: Turn `all_train` to `False` in next cell and run everything except sub_A sections"]},{"metadata":{"id":"Njtqa_6yiDoX","colab_type":"text"},"cell_type":"markdown","source":["## setup: downloading models, importing packages, util functions"]},{"metadata":{"id":"SyP2MCyobLrF","colab_type":"code","colab":{}},"cell_type":"code","source":["all_train = True  # True for sub_A, False for sub_B"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KSq6z0oFVRDi","colab_type":"code","colab":{}},"cell_type":"code","source":["path = 'drive/My Drive/pronoun/'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nV-lKWhrMFMB","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","import zipfile\n","import sys\n","import datetime\n","from glob import glob\n","import gc\n","from tqdm import tqdm\n","import shutil\n","import re"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WizEEkuBdDA4","colab_type":"code","colab":{}},"cell_type":"code","source":["input_tsv = path+'input/test_stage_2.tsv'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8ZRGsvT7kOdB","colab_type":"text"},"cell_type":"markdown","source":["## End2end model"]},{"metadata":{"id":"aeJ5TllvdBAX","colab_type":"text"},"cell_type":"markdown","source":["### load previously extraced features"]},{"metadata":{"id":"3bZSELrkcLE5","colab_type":"code","colab":{}},"cell_type":"code","source":["use_lingui_features = True if all_train else False"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9W2AYTwI3Yvq","colab_type":"code","colab":{}},"cell_type":"code","source":["def create_input(embed_df, dist_df):\n","    \n","    if len(embed_df) != len(dist_df): print(len(embed_df), len(dist_df))\n","    assert len(embed_df) == len(dist_df)\n","    all_P, all_A, all_B = [] ,[] ,[]\n","    all_label = []\n","    all_dist_PA, all_dist_PB = [], []    \n","    \n","    for i in range(len(embed_df)):\n","        \n","        all_P.append(embed_df.loc[i, \"emb_P\"])\n","        all_A.append(embed_df.loc[i, \"emb_A\"])\n","        all_B.append(embed_df.loc[i, \"emb_B\"])\n","        all_dist_PA.append(dist_df.loc[i, \"D_PA\"])\n","        all_dist_PB.append(dist_df.loc[i, \"D_PB\"])                \n","\n","    result_lst = [np.asarray(all_A), np.asarray(all_B), np.asarray(all_P),\n","                  np.expand_dims(np.asarray(all_dist_PA),axis=1),\n","                  np.expand_dims(np.asarray(all_dist_PB),axis=1)]\n","\n","    if use_lingui_features:\n","      for col in dist_df.columns[2:].values:\n","        result_lst.append(np.expand_dims(dist_df[col].values,axis=1))\n","            \n","    return result_lst, all_label "],"execution_count":0,"outputs":[]},{"metadata":{"id":"FN7fuIwidNxz","colab_type":"code","colab":{}},"cell_type":"code","source":["# load previously extracted stage2 features\n","\n","LARGE = True\n","\n","def load_stage2_features(CASED):\n","\n","  layer = \"-4\"\n","  \n","  suffix = layer\n","  if CASED: suffix += '_CASED'\n","  if LARGE: suffix += '_LARGE'\n","\n","  json_suffix = '_fix_long_text.json'\n","\n","  TTA_suffixes = [ \\\n","                   '_Alice_Kate_John_Michael',\n","                   '_Elizabeth_Mary_James_Henry',\n","                   '_Kate_Elizabeth_Michael_James',\n","                   '_Mary_Alice_Henry_John']\n","\n","  d_X_test = {}     # dict for test features\n","\n","  for TTA_suffix in [''] + TTA_suffixes:\n","    gc.collect()\n","\n","    num_test = pd.read_csv(input_tsv,sep='\\t').shape[0]\n","    n_chunk = int(np.ceil(num_test/1000))\n","    for i in range(n_chunk):    \n","      df_stage2_chunk =  pd.read_json(path+'output/contextual_embeddings_'+ os.path.basename(input_tsv).split('.')[0] + '_' + suffix +TTA_suffix+ f'_{i}'+ json_suffix).sort_index()  \n","      if i==0: df_stage2 = df_stage2_chunk.copy()\n","      else: df_stage2 = pd.concat([df_stage2,df_stage2_chunk])\n","      \n","    stage2_emb0 = df_stage2.reset_index(drop=True).copy()  \n","\n","    stage2_dist_df = pd.read_csv(path+'output/stage2_dist_df.csv').reset_index(drop=True).copy()\n","    if use_lingui_features:\n","      stage2_lingui_df = pd.read_csv(path+'output/stage2_lingui_df.csv')\n","      stage2_dist_df = pd.concat([pd.read_csv(path+'output/stage2_dist_df.csv')[['D_PA','D_PB']], stage2_lingui_df], axis=1)\n","\n","    # put into dictionary\n","    key = 'orig' if TTA_suffix=='' else TTA_suffix.strip('_')    \n","\n","    X_test0, _ = create_input(stage2_emb0, stage2_dist_df)    \n","    d_X_test[key] = X_test0.copy()\n","\n","  return d_X_test"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pwT3EpzyOMvf","colab_type":"text"},"cell_type":"markdown","source":["### keras model"]},{"metadata":{"id":"kvjVj8JkkkA0","colab_type":"code","outputId":"884bec3e-29d5-4d61-c6ba-d0ac53580893","executionInfo":{"status":"ok","timestamp":1555473350465,"user_tz":240,"elapsed":2524,"user":{"displayName":"Bo Liu","photoUrl":"https://lh5.googleusercontent.com/-yAtjaCoYe44/AAAAAAAAAAI/AAAAAAAAAE8/Jc2yMe60vOc/s64/photo.jpg","userId":"00951788254292121969"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["from keras.layers import *\n","import keras.backend as K\n","from keras.models import *\n","import keras\n","from keras import optimizers\n","from keras import callbacks\n","from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot\n","\n","class End2End_NCR():\n","    \n","    def __init__(self, word_input_shape, dist_shape, embed_dim=20): \n","        \n","        self.word_input_shape = word_input_shape\n","        self.dist_shape   = dist_shape\n","        self.embed_dim    = embed_dim\n","        self.buckets      = [1, 2, 3, 4, 5, 8, 16, 32, 64] \n","        self.hidden_dim   = 150\n","        self.dense_layer_sizes = [512,32]\n","        self.dropout_rate = 0.6\n","        \n","    def build(self):\n","        \n","        A, B, P = Input((self.word_input_shape,)), Input((self.word_input_shape,)), Input((self.word_input_shape,))\n","        inputs = [A, B, P]\n","        if use_lingui_features: \n","          num_lingui_features = pd.read_csv(path+'output/stage2_lingui_df.csv').shape[1]\n","          dist_inputs = [Input((1,)) for i in range(num_lingui_features+2)]\n","        else: \n","          dist1, dist2 = Input((self.dist_shape,)), Input((self.dist_shape,))\n","          dist_inputs = [dist1, dist2]\n","        \n","        self.dist_embed = Embedding(10, self.embed_dim)\n","        self.ffnn       = Sequential([Dense(self.hidden_dim, use_bias=True),\n","                                     Activation('relu'),\n","                                     Dropout(rate=0.2, seed = 7),\n","                                     Dense(1, activation='linear')])              \n","        \n","        dist_embeds = [self.dist_embed(dist) for dist in dist_inputs[:2]]\n","        dist_embeds = [Flatten()(dist_embed) for dist_embed in dist_embeds]\n","        \n","        #Scoring layer\n","        #In https://www.aclweb.org/anthology/D17-1018, \n","        #used feed forward network which measures if it is an entity mention using a score\n","        #because we already know the word is mention.\n","        #In here, I just focus on the pairwise score\n","        PA = Multiply()([inputs[0], inputs[2]])\n","        PB = Multiply()([inputs[1], inputs[2]])\n","        #PairScore: sa(i,j) =wa·FFNNa([gi,gj,gi◦gj,φ(i,j)])\n","        # gi is embedding of Pronoun\n","        # gj is embedding of A or B\n","        # gi◦gj is element-wise multiplication\n","        # φ(i,j) is the distance embedding\n","        if use_lingui_features:\n","          PA = Concatenate(axis=-1)([P, A, PA, dist_embeds[0]] + [dist_inputs[i] for i in [2,3,4,5,6]])\n","          PB = Concatenate(axis=-1)([P, B, PB, dist_embeds[1]] + [dist_inputs[i] for i in [7,8,9,10,11]])\n","        else:\n","          PA = Concatenate(axis=-1)([P, A, PA, dist_embeds[0]])\n","          PB = Concatenate(axis=-1)([P, B, PB, dist_embeds[1]])\n","        PA_score = self.ffnn(PA)\n","        PB_score = self.ffnn(PB)\n","        # Fix the Neither to score 0.\n","        score_e  = Lambda(lambda x: K.zeros_like(x))(PB_score)\n","        \n","        #Final Output\n","        output = Concatenate(axis=-1)([PA_score, PB_score, score_e]) # [Pronoun and A score, Pronoun and B score, Neither Score]\n","        output = Activation('softmax')(output)        \n","        model = Model(inputs+dist_inputs, output)\n","        \n","        return model"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"bzs3pz-6OOtR","colab_type":"text"},"cell_type":"markdown","source":["### inference"]},{"metadata":{"id":"rd8e33HZy2Xm","colab_type":"text"},"cell_type":"markdown","source":["#### sub_A (all_train)"]},{"metadata":{"id":"7aniGK_yzMnY","colab_type":"code","outputId":"72d24b3e-924d-43e9-d294-e5763c3fa942","executionInfo":{"status":"ok","timestamp":1555474869548,"user_tz":240,"elapsed":1518953,"user":{"displayName":"Bo Liu","photoUrl":"https://lh5.googleusercontent.com/-yAtjaCoYe44/AAAAAAAAAAI/AAAAAAAAAE8/Jc2yMe60vOc/s64/photo.jpg","userId":"00951788254292121969"}},"colab":{"base_uri":"https://localhost:8080/","height":292}},"cell_type":"code","source":["%%time \n","\n","if all_train:\n","  \n","  n_fold = 5\n","  n_run = 5\n","  num_test = pd.read_csv(input_tsv,sep='\\t').shape[0]\n","\n","  TTA_suffixes = \\\n","  ['Alice_Kate_John_Michael',\n","   'Elizabeth_Mary_James_Henry',\n","   'Kate_Elizabeth_Michael_James',\n","   'Mary_Alice_Henry_John',\n","   'orig']\n","\n","  pred_ensemble_end2end = np.zeros((num_test,3))\n","\n","  for CASED in [False,True]:\n","    gc.collect()\n","\n","    d_X_test = load_stage2_features(CASED)\n","    \n","    model = End2End_NCR(word_input_shape=d_X_test['orig'][0].shape[1], dist_shape=d_X_test['orig'][3].shape[1]).build()\n","\n","    if CASED: \n","      wts_prefix = path + 'wts/e2e-4_CASED_LARGE_Aug4_all_train_4400_Lingui_10_'\n","      ensemble_wts = [0.15, 0.2, 0.35, 0.2, 0.1]\n","    else:     \n","      wts_prefix = path + 'wts/e2e-4_LARGE_Aug4_all_train_4400_Lingui_10_'\n","      ensemble_wts = [0.25, 0.2, 0.3, 0.15, 0.1]\n","\n","    pred_all_d = {} # to save 125 fold avg (for Test), 5 runs, 5 outer OOF, 5 inner early stop val\n","    for TTA_suffix in TTA_suffixes: pred_all_d[TTA_suffix] = np.zeros((num_test,3))     \n","\n","    print('------ start inference ------')\n","\n","    for run in tqdm(range(n_run)):  \n","      for fold in range(n_fold):\n","        for fold_inner in range(n_fold):\n","          wts = wts_prefix + f'{run}{fold}{fold_inner}.hdf5'\n","          model.load_weights(wts)\n","          for TTA_suffix in TTA_suffixes:   \n","            pred = model.predict(x = d_X_test[TTA_suffix], verbose = 0)    \n","            pred_all_d[TTA_suffix] += pred / n_fold / n_fold / n_run \n","\n","\n","    pred_ensemble = np.zeros((num_test,3))    \n","    print(ensemble_wts)\n","    for i,TTA_suffix in enumerate(TTA_suffixes):    \n","      pred_ensemble += ensemble_wts[i]*pred_all_d[TTA_suffix]\n","\n","    if not CASED: pred_ensemble_end2end += pred_ensemble * 0.4\n","    else:         pred_ensemble_end2end += pred_ensemble * 0.6\n","\n","\n","  assert pred_ensemble_end2end.sum(axis=1).min() > 0.999 and pred_ensemble_end2end.sum(axis=1).max() < 1.001    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["------ start inference ------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 5/5 [10:27<00:00, 125.26s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["[0.25, 0.2, 0.3, 0.15, 0.1]\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["------ start inference ------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 5/5 [10:34<00:00, 126.97s/it]"],"name":"stderr"},{"output_type":"stream","text":["[0.15, 0.2, 0.35, 0.2, 0.1]\n","CPU times: user 23min 39s, sys: 2min 35s, total: 26min 15s\n","Wall time: 25min 17s\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"metadata":{"id":"I60REo7JSdC8","colab_type":"code","colab":{}},"cell_type":"code","source":["## read stage2 sample submission and write output csv \n","\n","sub = pd.read_csv(path+'input/sample_submission_stage_2.csv')\n","\n","\n","sub_end2end = sub.copy()\n","sub_end2end[['A','B','NEITHER']] = pred_ensemble_end2end\n","\n","out_csv_path = path + 'sub/sub_end2end_' + os.path.basename(input_tsv).split('.')[0] + '.csv'\n","if os.path.exists(out_csv_path): os.remove(out_csv_path)\n","sub_end2end.to_csv(out_csv_path, index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kedMVDQey8L5","colab_type":"text"},"cell_type":"markdown","source":["#### sub_B (not all_train)"]},{"metadata":{"id":"pRcM4eHny-NJ","colab_type":"code","outputId":"3127c6ab-908d-4b6b-aaeb-1eede50b17a6","executionInfo":{"status":"ok","timestamp":1555472874437,"user_tz":240,"elapsed":263240,"user":{"displayName":"Fangzhou Ma","photoUrl":"","userId":"10244018104853308786"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"cell_type":"code","source":["%%time\n","\n","if not all_train:\n","\n","  n_fold = 5\n","  n_run = 5\n","  num_test = pd.read_csv(input_tsv,sep='\\t').shape[0]\n","\n","  TTA_suffixes = \\\n","  ['Alice_Kate_John_Michael',\n","   'Elizabeth_Mary_James_Henry',\n","   'Kate_Elizabeth_Michael_James',\n","   'Mary_Alice_Henry_John',\n","   'orig']\n","\n","  pred_ensemble_end2endB = np.zeros((num_test,3))\n","\n","  for CASED in [False,True]:\n","    gc.collect()\n","\n","    d_X_test = load_stage2_features(CASED)\n","    \n","    model = End2End_NCR(word_input_shape=d_X_test['orig'][0].shape[1], dist_shape=d_X_test['orig'][3].shape[1]).build()\n","\n","    if CASED: \n","      wts_prefix = path + 'wts/e2e-4_CASED_LARGE_Aug4_sub_B_4400_'\n","      ensemble_wts = [0.2, 0.2, 0.4, 0.1, 0.1]\n","    else:     \n","      wts_prefix = path + 'wts/e2e-4_LARGE_Aug4_sub_B_4400_'\n","      ensemble_wts = [0.2, 0.2, 0.4, 0.0, 0.2]\n","\n","    pred_all_d = {} \n","    for TTA_suffix in TTA_suffixes: pred_all_d[TTA_suffix] = np.zeros((num_test,3))     \n","\n","    print('------ start inference ------')\n","\n","    for run in range(n_run):  \n","      for fold in range(n_fold):\n","        wts = wts_prefix + f'{run}{fold}.hdf5'\n","        model.load_weights(wts)\n","        for TTA_suffix in TTA_suffixes:   \n","          pred = model.predict(x = d_X_test[TTA_suffix], verbose = 0)    \n","          pred_all_d[TTA_suffix] += pred / n_fold / n_run         \n","\n","    pred_ensemble = np.zeros((num_test,3))    \n","    print(ensemble_wts)\n","    for i,TTA_suffix in enumerate(TTA_suffixes):    \n","      pred_ensemble += ensemble_wts[i]*pred_all_d[TTA_suffix]\n","\n","    if not CASED: pred_ensemble_end2endB += pred_ensemble * 0.4\n","    else:         pred_ensemble_end2endB += pred_ensemble * 0.6\n","\n","\n","  assert pred_ensemble_end2endB.sum(axis=1).min() > 0.999 and pred_ensemble_end2endB.sum(axis=1).max() < 1.001    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["------ start inference ------\n","[0.2, 0.2, 0.4, 0.0, 0.2]\n","------ start inference ------\n","[0.2, 0.2, 0.4, 0.1, 0.1]\n","CPU times: user 4min 52s, sys: 40.4 s, total: 5min 32s\n","Wall time: 4min 22s\n"],"name":"stdout"}]},{"metadata":{"id":"XOnVSOF_2emt","colab_type":"code","colab":{}},"cell_type":"code","source":["## read stage2 sample submission and write output csv \n","\n","sub = pd.read_csv(path+'input/sample_submission_stage_2.csv')\n","\n","\n","subB_end2end = sub.copy()\n","subB_end2end[['A','B','NEITHER']] = pred_ensemble_end2endB\n","\n","out_csv_path = path + 'sub/subB_end2end_' + os.path.basename(input_tsv).split('.')[0] + '.csv'\n","if os.path.exists(out_csv_path): os.remove(out_csv_path)\n","subB_end2end.to_csv(out_csv_path, index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RcnRXepCiqsZ","colab_type":"text"},"cell_type":"markdown","source":["## pure bert model"]},{"metadata":{"id":"NY-otVWUgFAt","colab_type":"text"},"cell_type":"markdown","source":["### load previously extraced features"]},{"metadata":{"id":"E22hScEWQ42N","colab_type":"code","colab":{}},"cell_type":"code","source":["def parse_json(embeddings):\n","\t'''\n","\tParses the embeddigns given by BERT, and suitably formats them to be passed to the MLP model\n","\n","\tInput: embeddings, a DataFrame containing contextual embeddings from BERT, as well as the labels for the classification problem\n","\tcolumns: \"emb_A\": contextual embedding for the word A\n","\t         \"emb_B\": contextual embedding for the word B\n","\t         \"emb_P\": contextual embedding for the pronoun\n","\t         \"label\": the answer to the coreference problem: \"A\", \"B\" or \"NEITHER\"\n","\n","\tOutput: X, a numpy array containing, for each line in the GAP file, the concatenation of the embeddings of the target words\n","\t        Y, a numpy array containing, for each line in the GAP file, the one-hot encoded answer to the coreference problem\n","\t'''\n","\tembeddings.sort_index(inplace = True) # Sorting the DataFrame, because reading from the json file messed with the order\n","\tnum_token = 3\n","\tBS = 768 if not LARGE else 1024\n","\tX = np.zeros((len(embeddings),num_token*BS)) \n","\n","\t# Concatenate features\n","\tfor i in range(len(embeddings)):\n","\t\tA = np.array(embeddings.loc[i,\"emb_A\"])\n","\t\tB = np.array(embeddings.loc[i,\"emb_B\"])\n","\t\tP = np.array(embeddings.loc[i,\"emb_P\"])\n","\t\tX[i] = np.concatenate((A,B,P))\n","         \n","\treturn X\n","\n","\n","def make_np_features_from_json(CASED = True,\n","                               LARGE = True,\n","                               MAX_SEQ_LEN = 256,\n","                               layer = None,\n","                               concat_lst = [\"-3\",\"-4\"],\n","                               TTA_suffix = ''\n","                              ):  \n","  # single layer\n","  if concat_lst == None:\n","    suffix = ''\n","    if CASED: suffix += '_CASED'\n","    if LARGE: suffix += '_LARGE'   \n","\n","    num_test = pd.read_csv(input_tsv,sep='\\t').shape[0]\n","    n_chunk = int(np.ceil(num_test/1000))\n","    for i in range(n_chunk):     \n","      json_name = path + 'output/contextual_embeddings_' + os.path.basename(input_tsv).split('.')[0] + '_' + layer+ suffix +TTA_suffix+  f\"_{i}\" +'_fix_long_text.json'\n","      stage2_emb_chunk = pd.read_json(json_name).sort_index()\n","      if i==0: stage2_emb = stage2_emb_chunk.copy()\n","      else:    stage2_emb = pd.concat([stage2_emb,stage2_emb_chunk])\n","    stage2_emb = stage2_emb.reset_index(drop=True).copy()     \n","    X_stage2 = parse_json(stage2_emb)   \n","                               \n","  # concat, recursive\n","  else:   \n","    for this_layer in concat_lst:      \n","      # recursive\n","      X_stage2_layer = \\\n","          make_np_features_from_json(CASED, LARGE, MAX_SEQ_LEN, this_layer, None, TTA_suffix)\n","\n","      if this_layer==concat_lst[0]:\n","        X_stage2 = X_stage2_layer\n","      else:\n","        X_stage2 = np.concatenate((X_stage2,X_stage2_layer),axis=1)  \n","    \n","  return X_stage2               "],"execution_count":0,"outputs":[]},{"metadata":{"id":"TcbhU8ikeWY9","colab_type":"code","colab":{}},"cell_type":"code","source":["LARGE = True\n","\n","concat_lst = [\"-3\",\"-4\"]\n","layer = None \n","MAX_SEQ_LEN = 256\n","\n","TTA_suffixes = [\\\n","                 'Alice_Kate_John_Michael',\n","                 'Elizabeth_Mary_James_Henry',\n","                 'Kate_Elizabeth_Michael_James',\n","                 'Mary_Alice_Henry_John']\n","\n","def load_pure_bert_stage2_features(CASED):\n","\n","  d_XY = {}\n","\n","  for TTA_suffix in ['orig'] + TTA_suffixes:\n","    this_d = {}\n","\n","    this_d['X_stage2'] = \\\n","            make_np_features_from_json(CASED = CASED,\n","                                       LARGE = LARGE,\n","                                       MAX_SEQ_LEN = MAX_SEQ_LEN,\n","                                       layer = layer,\n","                                       concat_lst = concat_lst,\n","                                       TTA_suffix = '' if TTA_suffix=='orig' else '_'+TTA_suffix)   \n","    d_XY[TTA_suffix] = this_d\n","\n","  return d_XY    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZDV3FfN9OSFe","colab_type":"text"},"cell_type":"markdown","source":["### keras model"]},{"metadata":{"id":"SBoJTlSwqH8o","colab_type":"code","colab":{}},"cell_type":"code","source":["## keras model\n","\n","from keras import backend, models, layers, initializers, regularizers, constraints, optimizers\n","from keras import callbacks as kc\n","from keras import optimizers as ko\n","\n","from sklearn.model_selection import cross_val_score, KFold, train_test_split\n","from sklearn.metrics import log_loss\n","import time\n","\n","dense_layer_sizes = [512,32]\n","dropout_rate = 0.6\n","lambd = 0.1 # L2 regularization\n","\n","\n","def build_mlp_model(input_shape):\n","\tX_input = layers.Input(input_shape)\n","\n","\t# First dense layer\n","\tX = layers.Dense(dense_layer_sizes[0], name = 'dense0')(X_input)\n","\tX = layers.BatchNormalization(name = 'bn0')(X)\n","\tX = layers.Activation('relu')(X)\n","\tX = layers.Dropout(dropout_rate, seed = 7)(X)\n","\n","\t# Second dense layer\n","\tif len(dense_layer_sizes)==2:\n","\t\tX = layers.Dense(dense_layer_sizes[1], name = 'dense1')(X)\n","\t\tX = layers.BatchNormalization(name = 'bn1')(X)\n","\t\tX = layers.Activation('relu')(X)\n","\t\tX = layers.Dropout(dropout_rate, seed = 9)(X)\n","\n","\t# Output layer\n","\tX = layers.Dense(3, name = 'output', kernel_regularizer = regularizers.l2(lambd))(X)\n","\tX = layers.Activation('softmax')(X)\n","\n","\t# Create model\n","\tmodel = models.Model(input = X_input, output = X, name = \"classif_model\")\n","\treturn model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ExozpNfEOUhH","colab_type":"text"},"cell_type":"markdown","source":["### inference"]},{"metadata":{"id":"Ni2AEXBpVLRy","colab_type":"text"},"cell_type":"markdown","source":["#### sub_A (all_train)"]},{"metadata":{"id":"kgPOJPnb6wKe","colab_type":"code","outputId":"8e260183-c319-4f9b-eb13-8a97ab0d7d9b","executionInfo":{"status":"ok","timestamp":1555475807112,"user_tz":240,"elapsed":651649,"user":{"displayName":"Bo Liu","photoUrl":"https://lh5.googleusercontent.com/-yAtjaCoYe44/AAAAAAAAAAI/AAAAAAAAAE8/Jc2yMe60vOc/s64/photo.jpg","userId":"00951788254292121969"}},"colab":{"base_uri":"https://localhost:8080/","height":165}},"cell_type":"code","source":["%%time\n","\n","if all_train:\n","  model = build_mlp_model([1024*3*2])\n","\n","\n","  TTA_suffixes = \\\n","  ['Alice_Kate_John_Michael',\n","   'Elizabeth_Mary_James_Henry',\n","   'Kate_Elizabeth_Michael_James',\n","   'Mary_Alice_Henry_John',\n","   'orig']\n","\n","  pred_ensemble_pure_bert = np.zeros((num_test,3))\n","\n","  for CASED in [False,True]:\n","    gc.collect()\n","\n","    d_XY = load_pure_bert_stage2_features(CASED)\n","\n","    if CASED: \n","      wts_prefix = path + 'wts/pure_bertbert_-3-4_CASED_LARGE_Aug3_fix85_all_train_4400_'\n","      ensemble_wts = [0.25, 0, 0.4, 0.25, 0.1]\n","    else:     \n","      wts_prefix = path + 'wts/pure_bertbert_-3-4_LARGE_Aug3_fix85_all_train_4400_'\n","      ensemble_wts = [0.3, 0.1, 0.3, 0.2, 0.1]\n","\n","    pred_all_d = {} \n","    for TTA_suffix in TTA_suffixes: pred_all_d[TTA_suffix] = np.zeros((num_test,3))     \n","\n","    print('------ start inference ------')\n","\n","    for fold in range(n_fold):\n","      for fold_inner in range(n_fold):\n","        wts = wts_prefix + f'0{fold}{fold_inner}.hdf5'\n","        model.load_weights(wts)\n","        for TTA_suffix in TTA_suffixes:   \n","          pred = model.predict(x = d_XY[TTA_suffix]['X_stage2'], verbose = 0)\n","          pred_all_d[TTA_suffix] += pred / n_fold / n_fold \n","\n","    pred_ensemble = np.zeros((num_test,3))    \n","    print(ensemble_wts)\n","    for i,TTA_suffix in enumerate(TTA_suffixes):    \n","      pred_ensemble += ensemble_wts[i]*pred_all_d[TTA_suffix]     \n","\n","    if not CASED: pred_ensemble_pure_bert += pred_ensemble * 0.4\n","    else:         pred_ensemble_pure_bert += pred_ensemble * 0.6\n","\n","\n","  assert pred_ensemble_pure_bert.sum(axis=1).min() > 0.999 and pred_ensemble_pure_bert.sum(axis=1).max() < 1.001    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"],"name":"stderr"},{"output_type":"stream","text":["------ start inference ------\n","[0.3, 0.1, 0.3, 0.2, 0.1]\n","------ start inference ------\n","[0.25, 0, 0.4, 0.25, 0.1]\n","CPU times: user 6min 5s, sys: 46.8 s, total: 6min 52s\n","Wall time: 10min 50s\n"],"name":"stdout"}]},{"metadata":{"id":"O9YrYiH9NxVN","colab_type":"code","colab":{}},"cell_type":"code","source":["## read stage2 sample submission and write output csv \n","\n","sub = pd.read_csv(path+'input/sample_submission_stage_2.csv')\n","\n","\n","sub_pure_bert = sub.copy()\n","sub_pure_bert[['A','B','NEITHER']] = pred_ensemble_pure_bert\n","\n","out_csv_path = path + 'sub/sub_pure_bert_' + os.path.basename(input_tsv).split('.')[0] + '.csv'\n","if os.path.exists(out_csv_path): os.remove(out_csv_path)\n","sub_pure_bert.to_csv(out_csv_path, index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u6Mztr-RVQ-a","colab_type":"text"},"cell_type":"markdown","source":["#### sub_B (not all_train)"]},{"metadata":{"id":"eSTk8dsTVSvI","colab_type":"code","outputId":"afe4d302-982f-49f8-baed-b679defb38ca","executionInfo":{"status":"ok","timestamp":1555472369680,"user_tz":240,"elapsed":920984,"user":{"displayName":"Fangzhou Ma","photoUrl":"","userId":"10244018104853308786"}},"colab":{"base_uri":"https://localhost:8080/","height":166}},"cell_type":"code","source":["%%time\n","\n","if not all_train:\n","  \n","  n_run = 5\n","  n_fold = 5\n","  num_test = pd.read_csv(input_tsv,sep='\\t').shape[0]\n","  \n","  model = build_mlp_model([1024*3*2])\n","\n","  TTA_suffixes = \\\n","  ['Alice_Kate_John_Michael',\n","   'Elizabeth_Mary_James_Henry',\n","   'Kate_Elizabeth_Michael_James',\n","   'Mary_Alice_Henry_John',\n","   'orig']\n","\n","  pred_ensemble_pure_bertB = np.zeros((num_test,3))\n","\n","  for CASED in [False,True]:\n","    gc.collect()\n","\n","    d_XY = load_pure_bert_stage2_features(CASED)\n","\n","    if CASED: \n","      wts_prefix = path + 'wts/pure_bertpure_bert_-3-4_CASED_LARGE_Aug3_sub_B_4400_'\n","      ensemble_wts = [0.25, 0.0, 0.4, 0.25, 0.1]\n","    else:     \n","      wts_prefix = path + 'wts/pure_bertpure_bert_-3-4_LARGE_Aug3_sub_B_4400_'\n","      ensemble_wts = [0.3, 0.1, 0.3, 0.2, 0.1]\n","\n","    pred_all_d = {} \n","    for TTA_suffix in TTA_suffixes: pred_all_d[TTA_suffix] = np.zeros((num_test,3))     \n","\n","    print('------ start inference ------')\n","\n","    for run in range(n_run):  \n","      for fold in range(n_fold):\n","        wts = wts_prefix + f'{run}{fold}.hdf5'\n","        model.load_weights(wts)\n","        for TTA_suffix in TTA_suffixes:   \n","          pred = model.predict(x = d_XY[TTA_suffix]['X_stage2'], verbose = 0)\n","          pred_all_d[TTA_suffix] += pred / n_fold / n_run\n","\n","    pred_ensemble = np.zeros((num_test,3))    \n","    print(ensemble_wts)\n","    for i,TTA_suffix in enumerate(TTA_suffixes):    \n","      pred_ensemble += ensemble_wts[i]*pred_all_d[TTA_suffix]    \n","\n","    if not CASED: pred_ensemble_pure_bertB += pred_ensemble * 0.5\n","    else:         pred_ensemble_pure_bertB += pred_ensemble * 0.5\n","\n","\n","  assert pred_ensemble_pure_bertB.sum(axis=1).min() > 0.999 and pred_ensemble_pure_bertB.sum(axis=1).max() < 1.001    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"classif_model\", inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"],"name":"stderr"},{"output_type":"stream","text":["------ start inference ------\n","[0.3, 0.1, 0.3, 0.2, 0.1]\n","------ start inference ------\n","[0.25, 0.0, 0.4, 0.25, 0.1]\n","CPU times: user 6min 17s, sys: 8min 22s, total: 14min 40s\n","Wall time: 15min 20s\n"],"name":"stdout"}]},{"metadata":{"id":"2AlFaTAstCbj","colab_type":"code","colab":{}},"cell_type":"code","source":["## read stage2 sample submission and write output csv \n","\n","sub = pd.read_csv(path+'input/sample_submission_stage_2.csv')\n","\n","\n","subB_pure_bert = sub.copy()\n","subB_pure_bert[['A','B','NEITHER']] = pred_ensemble_pure_bertB\n","\n","out_csv_path = path + 'sub/subB_pure_bert_' + os.path.basename(input_tsv).split('.')[0] + '.csv'\n","if os.path.exists(out_csv_path): os.remove(out_csv_path)\n","subB_pure_bert.to_csv(out_csv_path, index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"psZ0nu2sPqCe","colab_type":"text"},"cell_type":"markdown","source":["## ensemble end2end and pure_bert models"]},{"metadata":{"id":"nyrrVB-tZkhH","colab_type":"text"},"cell_type":"markdown","source":["### sub_A"]},{"metadata":{"id":"chj_kD_9Pth_","colab_type":"code","colab":{}},"cell_type":"code","source":["if all_train:\n","  pred_two_model_A = pred_ensemble_end2end * 0.9 + pred_ensemble_pure_bert * 0.1\n","\n","  pred_two_model_A = np.clip(pred_two_model_A, 0.005, None)\n","\n","  sub_two_model = sub.copy()\n","  sub_two_model[['A','B','NEITHER']] = pred_two_model_A\n","\n","  out_csv_path = path + 'sub/sub_two_model_' + os.path.basename(input_tsv).split('.')[0] + '.csv'\n","  if os.path.exists(out_csv_path): os.remove(out_csv_path)\n","  sub_two_model.to_csv(out_csv_path, index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"y9LityFwZmhM","colab_type":"text"},"cell_type":"markdown","source":["### sub_B"]},{"metadata":{"id":"Aii6b29XtXjD","colab_type":"code","colab":{}},"cell_type":"code","source":["if not all_train:\n","\n","  pred_two_model_B = pred_ensemble_end2endB * 0.8 + pred_ensemble_pure_bertB * 0.2\n","\n","  pred_two_model_B = np.clip(pred_two_model_B, 0.006, None)\n","\n","  sub_two_model_B = sub.copy()\n","  sub_two_model_B[['A','B','NEITHER']] = pred_two_model_B\n","\n","  out_csv_path = path + 'sub/subB_two_model_' + os.path.basename(input_tsv).split('.')[0] + '.csv'\n","  if os.path.exists(out_csv_path): os.remove(out_csv_path)\n","  sub_two_model_B.to_csv(out_csv_path, index=False)  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"C_Bzgmw1eXhv","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}