{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Step1-preprocessing.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"BzMpIp4CVye3","colab_type":"text"},"cell_type":"markdown","source":["**pre-processing: given [dev, test, val, stage2] input tsv files:**\n","1. generate augmented tsv files\n","2. extract bert features and save as json files\n","3. calculate distance features and save as csv files\n","4. calculate linguistic features and save as csv files\n","\n","**Instructions**\n","* For training, set input_tsv path accordingly, then turn `is_inference` to `False` in next cell\n","* For inference, set `input_tsv` path accordingly, then turn `is_inference` to `True` in next cell"]},{"metadata":{"id":"Njtqa_6yiDoX","colab_type":"text"},"cell_type":"markdown","source":["## 0. setup: downloading models, importing packages, util functions"]},{"metadata":{"id":"t7pwBuS9P-g_","colab_type":"code","colab":{}},"cell_type":"code","source":["# turn this to True for inference; False for training\n","\n","is_inference = True"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KSq6z0oFVRDi","colab_type":"code","colab":{}},"cell_type":"code","source":["path = 'drive/My Drive/pronoun/'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jU4WrIH3maT4","colab_type":"code","colab":{}},"cell_type":"code","source":["input_tsv = path+'input/test_stage_2.tsv'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nV-lKWhrMFMB","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","import zipfile\n","import sys\n","import datetime\n","from glob import glob\n","import gc\n","from tqdm import tqdm\n","import shutil\n","import re\n","\n","!pip install gender-guesser\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Mg255_HUCgYV","colab_type":"code","colab":{}},"cell_type":"code","source":["!python -m spacy download en_core_web_lg\n","\n","import spacy\n","nlp = spacy.load('en_core_web_lg')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Pzu3s3vyD5D0","colab_type":"code","colab":{}},"cell_type":"code","source":["#downloading weights and cofiguration file for the model\n","\n","def get_bert_model(CASED, LARGE):\n","\n","  if CASED and LARGE:           model_name = 'cased_L-24_H-1024_A-16'\n","  elif not CASED and LARGE:     model_name = 'uncased_L-24_H-1024_A-16'\n","  elif CASED and not LARGE:     model_name = 'cased_L-12_H-768_A-12'\n","  elif not CASED and not LARGE: model_name = 'uncased_L-12_H-768_A-12'\n","\n","  if not os.path.exists(model_name + '.zip'):\n","\n","    if CASED and LARGE:           \n","      !wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip\n","    elif not CASED and LARGE:     \n","      !wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n","    elif CASED and not LARGE:     \n","      !wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n","    elif not CASED and not LARGE: \n","      !wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n","    \n","    with zipfile.ZipFile(model_name + '.zip',\"r\") as zip_ref:\n","        zip_ref.extractall()\n","    sorted(glob(model_name + '/*'))\n","    \n","\n","get_bert_model(CASED = False, LARGE = True)    \n","get_bert_model(CASED = True,  LARGE = True)    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"I1dEx93RMa9X","colab_type":"code","colab":{}},"cell_type":"code","source":["!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n","!wget https://raw.githubusercontent.com/google-research/bert/master/extract_features.py \n","!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n","  \n","!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\n","!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\n","!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv"],"execution_count":0,"outputs":[]},{"metadata":{"id":"21C4iJ9HdBIX","colab_type":"code","colab":{}},"cell_type":"code","source":["import modeling\n","import extract_features\n","import tokenization\n","import tensorflow as tf"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OfmF0yh2Ml4b","colab_type":"code","colab":{}},"cell_type":"code","source":["def compute_offset_no_spaces(text, offset):\n","\tcount = 0\n","\tfor pos in range(offset):\n","\t\tif text[pos] != \" \": count +=1\n","\treturn count\n","\n","def count_chars_no_special(text):\n","\tif text=='#': return 1  \n","\tcount = 0\n","\tspecial_char_list = [\"#\"]\n","\tfor pos in range(len(text)):\n","\t\tif text[pos] not in special_char_list: count +=1\n","\treturn count\n","\n","def count_length_no_special(text):\n","\tif text=='#': return 1\n","\tcount = 0\n","\tspecial_char_list = [\"#\", \" \"]\n","\tfor pos in range(len(text)):\n","\t\tif text[pos] not in special_char_list: count +=1\n","\treturn count"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Lllc6-69aPji","colab_type":"text"},"cell_type":"markdown","source":["## 1. generate augmented tsv files"]},{"metadata":{"id":"hYFVDIwVZee4","colab_type":"code","outputId":"715cb184-eb34-42d3-c844-973139e4508c","executionInfo":{"status":"ok","timestamp":1555445271834,"user_tz":240,"elapsed":288108,"user":{"displayName":"Bo Liu","photoUrl":"https://lh5.googleusercontent.com/-yAtjaCoYe44/AAAAAAAAAAI/AAAAAAAAAE8/Jc2yMe60vOc/s64/photo.jpg","userId":"00951788254292121969"}},"colab":{"base_uri":"https://localhost:8080/","height":163}},"cell_type":"code","source":["# for each input tsv file, generate 4 augmented tsv files and save to Drive\n","\n","\n","def generate_aug_files(input_tsv, output_prefix):\n","\n","   for names in [ { 'female':['Alice','Kate'], 'male': ['John','Michael']},\n","                  { 'female':['Elizabeth','Mary'], 'male': ['James','Henry']},\n","                  { 'female':['Kate','Elizabeth'], 'male': ['Michael','James']},\n","                  { 'female':['Mary','Alice'], 'male': ['Henry','John']}]:\n","    \n","        df = pd.read_csv(input_tsv, sep=\"\\t\")\n","    \n","        for i in tqdm(range(df.shape[0])):\n","                \n","            do_A = True\n","            do_B = True\n","        \n","            text = df.loc[i].Text\n","            A = df.loc[i].A\n","            B = df.loc[i].B\n","            P = df.loc[i].Pronoun\n","        \n","            if A in B or B in A:\n","                continue\n","            \n","            gender = 'female' if df.loc[i,'Pronoun'].lower() in ['her','she'] else 'male'        \n","            \n","            ## if placeholder names appear in text, skip to avoid confusion\n","            if names[gender][0] in text or names[gender][1] in text:\n","                continue\n","    \n","            # There are names like \"M\"\n","            if A in names[gender][0] or B in names[gender][0] or\\\n","               A in names[gender][1] or B in names[gender][1]:\n","                   continue\n","               \n","            ## if name too long, skip \n","            if len(A.split(' ')) > 2: do_A=False\n","            if len(B.split(' ')) > 2: do_B=False\n","            \n","            ## if either first or last name appearn with full name, skip\n","            if len(A.split(' '))==2:\n","                if text.count(A.split(' ')[0]) > text.count(A) or text.count(A.split(' ')[1]) > text.count(A):\n","                    do_A = False\n","            if len(B.split(' '))==2:\n","                if text.count(B.split(' ')[0]) > text.count(B) or text.count(B.split(' ')[1]) > text.count(B):\n","                    do_B = False\n","            if not do_A and not do_B: continue       \n","        \n","                        \n","            Aoff = df.loc[i,'A-offset']\n","            Boff = df.loc[i,'B-offset']\n","            Poff = df.loc[i,'Pronoun-offset']    \n","    \n","            if do_A:    \n","                while(A in text):\n","                    Apos = text.index(A)    \n","                    text = text.replace(A,names[gender][0] ,1)\n","                    if Apos < Aoff: Aoff += len(names[gender][0])-len(A)        \n","                    if Apos < Boff: Boff += len(names[gender][0])-len(A)\n","                    if Apos < Poff: Poff += len(names[gender][0])-len(A)\n","                df.loc[i,'A'] = names[gender][0]                    \n","    \n","            if do_B:        \n","                while(B in text):\n","                    Bpos = text.index(B)    \n","                    text = text.replace(B,names[gender][1] ,1)\n","                    if Bpos < Poff: Poff += len(names[gender][1])-len(B)\n","                    if Bpos < Boff: Boff += len(names[gender][1])-len(B)        \n","                    if Bpos < Aoff: Aoff += len(names[gender][1])-len(B)        \n","                df.loc[i,'B'] = names[gender][1] \n","        \n","            df.loc[i,'A-offset'] = Aoff\n","            df.loc[i,'B-offset'] = Boff\n","            df.loc[i,'Pronoun-offset'] = Poff\n","            df.loc[i,'Text'] = text\n","\n","        # sanity check\n","        for i in tqdm(range(df.shape[0])):        \n","            text = df.loc[i].Text\n","            A = df.loc[i].A\n","            B = df.loc[i].B\n","            P = df.loc[i].Pronoun\n","            Aoff = df.loc[i]['A-offset']\n","            Boff = df.loc[i]['B-offset']\n","            Poff = df.loc[i]['Pronoun-offset'] \n","            assert text[Aoff:(Aoff+len(A))]==A\n","            assert text[Boff:(Boff+len(B))]==B\n","            assert text[Poff:(Poff+len(P))]==P   \n","                    \n","        df.to_csv(output_prefix +\\\n","              '_'.join([names['female'][0],names['female'][1],names['male'][0],names['male'][1]]) +'.tsv', sep=\"\\t\",index=False) \n","    \n","\n","generate_aug_files(input_tsv, input_tsv.replace('.tsv', '_'))    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 12359/12359 [00:56<00:00, 220.02it/s]\n","100%|██████████| 12359/12359 [00:15<00:00, 806.00it/s]\n","100%|██████████| 12359/12359 [00:55<00:00, 223.86it/s]\n","100%|██████████| 12359/12359 [00:15<00:00, 790.41it/s]\n","100%|██████████| 12359/12359 [00:56<00:00, 218.91it/s]\n","100%|██████████| 12359/12359 [00:15<00:00, 792.95it/s]\n","100%|██████████| 12359/12359 [00:54<00:00, 228.44it/s]\n","100%|██████████| 12359/12359 [00:16<00:00, 757.25it/s]\n"],"name":"stderr"}]},{"metadata":{"id":"DJo04-yOcbnj","colab_type":"text"},"cell_type":"markdown","source":["## 2. extract bert features and save as json files"]},{"metadata":{"id":"shaXUnj6sgO5","colab_type":"text"},"cell_type":"markdown","source":["this section is based on https://www.kaggle.com/mateiionita/taming-the-bert-a-baseline with modifications"]},{"metadata":{"id":"iTAXRwRaDfXP","colab_type":"code","colab":{}},"cell_type":"code","source":["## adjust long text (cut off first few sentences so that last of A/B/Pronoun's offset < 900)\n","\n","def adjust_long_text(df_in):\n","  \n","  df = df_in.copy()\n","  try:    \n","    long_text_idx = df[df.apply(lambda x:max(x['A-offset'],x['B-offset'],x['Pronoun-offset']),axis=1) >= 1000].index.values\n","\n","    for i in long_text_idx:    \n","      text = df.loc[i].Text\n","      doc = nlp(text)\n","\n","      num_char_left = 800 if text.count('``')>5 else 1000\n","\n","      # number of char to be cut\n","      num_char_cut = df.apply(lambda x:max(x['A-offset'],x['B-offset'],x['Pronoun-offset']),axis=1).loc[i] - num_char_left\n","\n","      A = df.loc[i].A\n","      B = df.loc[i].B\n","      P = df.loc[i].Pronoun     \n","\n","      for _,sent in enumerate(list(doc.sents)):\n","        if sent.text in text:\n","          sent_start_idx = text.index(sent.text)\n","          if sent_start_idx > num_char_cut:\n","            break\n","\n","      if sent_start_idx >= min(df.loc[i,'A-offset'],df.loc[i,'B-offset'],df.loc[i,'Pronoun-offset']):\n","        continue\n","\n","      text = text[sent_start_idx:]\n","      Aoff = df.loc[i,'A-offset']-sent_start_idx\n","      Boff = df.loc[i,'B-offset']-sent_start_idx\n","      Poff = df.loc[i,'Pronoun-offset']-sent_start_idx\n","\n","      assert text[Aoff:(Aoff+len(A))]==A\n","      assert text[Boff:(Boff+len(B))]==B\n","      assert text[Poff:(Poff+len(P))]==P\n","\n","      df.loc[i,'A-offset'] = Aoff\n","      df.loc[i,'B-offset'] = Boff\n","      df.loc[i,'Pronoun-offset'] = Poff\n","      df.loc[i,'Text'] = text\n","\n","      print(f'adjusted index {i}')\n","      print(Aoff, Boff, Poff, len(text))\n","  except:\n","    pass\n","\n","  return df\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Wp-pQbAjMyVz","colab_type":"code","colab":{}},"cell_type":"code","source":["def run_bert(data, layer=\"-2\", LARGE=False,CASED=False, MAX_SEQ_LEN = 256, debug=False, is_inference=False):\n","\t'''\n","\tRuns a forward propagation of BERT on input text, extracting contextual word embeddings\n","\tInput: data, a pandas DataFrame containing the information in one of the GAP files\n","\n","\tOutput: emb, a pandas DataFrame containing contextual embeddings for the words A, B and Pronoun. Each embedding is a numpy array of shape (768)\n","\tcolumns: \"emb_A\": the embedding for word A\n","\t         \"emb_B\": the embedding for word B\n","\t         \"emb_P\": the embedding for the pronoun\n","\t         \"label\": the answer to the coreference problem: \"A\", \"B\" or \"NEITHER\"\n","\t'''\n","    # From the current file, take the text only, and write it in a file which will be passed to BERT\n","    \n","\tBS = 8    \n","\tif not CASED and not LARGE:\n","\t\tbert_zip_name = 'uncased_L-12_H-768_A-12'\n","\t\tSIZE = 768\n","\telif CASED and not LARGE:\n","\t\tbert_zip_name = 'cased_L-12_H-768_A-12'\n","\t\tSIZE = 768\n","\telif LARGE and not CASED:\n","\t\tbert_zip_name = 'uncased_L-24_H-1024_A-16'\n","\t\tSIZE = 1024\n","\t\tBS=4\n","\telif LARGE and CASED:\n","\t\tbert_zip_name = 'cased_L-24_H-1024_A-16'\n","\t\tSIZE = 1024\n","\t\tBS=4    \n","    \n","\tprint(bert_zip_name) \n","\n","\ttext = data[\"Text\"]\n","\ttext.to_csv(\"input.txt\", index = False, header = False)\n","\n","    # The script extract_features.py runs forward propagation through BERT, and writes the output in the file output.jsonl\n","\tcommand = \"python3 extract_features.py \\\n","\t  --input_file=input.txt \\\n","\t  --output_file=output.jsonl \\\n","\t  --vocab_file=\"+bert_zip_name+\"/vocab.txt \\\n","\t  --bert_config_file=\"+bert_zip_name+\"/bert_config.json \\\n","\t  --init_checkpoint=\"+bert_zip_name+\"/bert_model.ckpt \\\n","\t  --layers=\" + layer + \" \\\n","\t  --max_seq_length=\" + str(MAX_SEQ_LEN)+ \" \\\n","\t  --batch_size=\" + str(BS)\n","\tif CASED:\n","\t\tcommand += ' --do_lower_case=False'\n","\tprint(command)\n","\tos.system(command)\n","\n","\tbert_output = pd.read_json(\"output.jsonl\", lines = True)\n","\n","\tos.system(\"rm output.jsonl\")\n","\tos.system(\"rm input.txt\")\n","\n","\tdata.index = range(data.shape[0])\n","  \n","\tindex = data.index\n","\tcolumns = [\"emb_A\", \"emb_B\", \"emb_P\", \"label\"]\n","\temb = pd.DataFrame(index = index, columns = columns)\n","\temb.index.name = \"ID\"\n","\n","\tfor i in range(len(data)): # For each line in the data file\n","\t\t# get the words A, B, Pronoun. Convert them to lower case, since we're using the uncased version of BERT\n","\t\tif CASED:\n","\t\t\tP = data.loc[i,\"Pronoun\"]\n","\t\t\tA = data.loc[i,\"A\"]\n","\t\t\tB = data.loc[i,\"B\"]      \n","\t\telse:\n","\t\t\tP = data.loc[i,\"Pronoun\"].lower()\n","\t\t\tA = data.loc[i,\"A\"].lower()\n","\t\t\tB = data.loc[i,\"B\"].lower()\n","\n","\t\t# For each word, find the offset not counting spaces. This is necessary for comparison with the output of BERT\n","\t\tP_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"Pronoun-offset\"])\n","\t\tA_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"A-offset\"])\n","\t\tB_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"B-offset\"])\n","\t\t# Figure out the length of A, B, not counting spaces or special characters\n","\t\tA_length = count_length_no_special(A)\n","\t\tB_length = count_length_no_special(B)\n","\n","\t\t# Initialize embeddings with zeros\n","\t\temb_A = np.zeros(SIZE)\n","\t\temb_B = np.zeros(SIZE)\n","\t\temb_P = np.zeros(SIZE)\n","\n","    # Initialize counts\n","\t\tcount_chars = 0\n","\t\tcnt_A, cnt_B, cnt_P = 0, 0, 0\n","\t\ttoken_A, token_B, token_P = '','',''\n","\n","\t\tfeatures = pd.DataFrame(bert_output.loc[i,\"features\"]) # Get the BERT embeddings for the current line in the data file\n","\t\tstart_idx = 2 if features.loc[1,\"token\"] == '\"' else 1\n","\n","\t\tfor j in range(start_idx,len(features)):  # Iterate over the BERT tokens for the current line; we skip over the first 2 tokens, which don't correspond to words\n","\t\t\t\ttoken = features.loc[j,\"token\"]\n","\n","\t\t\t\t# See if the character count until the current token matches the offset of any of the 3 target words\n","\t\t\t\tif count_chars  == P_offset: \n","\t\t\t\t\t\ttoken_P += token.replace('#','')\n","\t\t\t\t\t\t# print(token)\n","\t\t\t\t\t\temb_P += np.array(features.loc[j,\"layers\"][0]['values'])\n","\t\t\t\t\t\tcnt_P += 1\n","\t\t\t\tif count_chars in range(A_offset, A_offset + A_length): \n","\t\t\t\t\t\ttoken_A += token.replace('#','')    \n","\t\t\t\t\t\t# print(token)\n","\t\t\t\t\t\temb_A += np.array(features.loc[j,\"layers\"][0]['values'])\n","\t\t\t\t\t\tcnt_A +=1  \n","\t\t\t\tif count_chars in range(B_offset, B_offset + B_length): \n","\t\t\t\t\t\ttoken_B += token.replace('#','')    \n","\t\t\t\t\t\t# print(token)\n","\t\t\t\t\t\temb_B += np.array(features.loc[j,\"layers\"][0]['values'])    \n","\t\t\t\t\t\tcnt_B +=1\t\t\t\t   \n","\t\t\t\t\t\t# Update the character count\n","\t\t\t\tcount_chars += count_length_no_special(token)\n","\t\tif not (token_A==A.replace(' ','') and token_B==B.replace(' ','') and token_P==P):\n","\t\t\t\tprint(\"assert failed for {:d}\".format(i))\n","\t\t\t\tprint(token_A,A.replace(' ','') , token_B,B.replace(' ','') , token_P,P)\n","\t\t# Taking the average between tokens in the span of A or B, so divide the current value by the count\t\n","\t\temb_A /= cnt_A\n","\t\temb_B /= cnt_B\n","    \n","\t\tif is_inference:\n","\t\t\tlabel = ''\n","\t\telse:\n","\t\t\t# Work out the label of the current piece of text\n","\t\t\tlabel = \"Neither\"\n","\t\t\tif (data.loc[i,\"A-coref\"] == True):\n","\t\t\t\tlabel = \"A\"\n","\t\t\tif (data.loc[i,\"B-coref\"] == True):\n","\t\t\t\tlabel = \"B\"\n","\n","\t\t# Put everything together in emb\n","\t\temb.iloc[i] = [emb_A, emb_B, emb_P, label]\n","      \n","\treturn emb"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CsE0v0Z3NEsv","colab_type":"code","colab":{}},"cell_type":"code","source":["### extract original features and 4 TTA features\n","\n","%%time\n","\n","LARGE = True\n","\n","\n","def extract_data(input_tsv_path, output_json_path, start_idx=None, end_idx=None, is_inference=False):\n","  if os.path.exists(path + 'output/contextual_embeddings_' + output_json_path):\n","    return\n","  data = pd.read_csv(path + 'input/' + input_tsv_path, sep = '\\t')\n","  if start_idx!=None and end_idx!=None:\n","    data = data.iloc[start_idx:end_idx]\n","  data = adjust_long_text(data)\n","  emb = run_bert(data, LARGE=LARGE, CASED=CASED,layer=layer, MAX_SEQ_LEN=MAX_SEQ_LEN, is_inference=is_inference)\n","  emb.to_json(path + 'output/contextual_embeddings_' + output_json_path, orient = 'columns')   \n","  gc.collect()\n","\n","\n","for CASED in [False]:  \n","  for layer in [\"-3\",\"-4\"]:\n","    print('----------- layer ' + layer )\n","    MAX_SEQ_LEN = 256\n","\n","    suffix = ('_'+ str(MAX_SEQ_LEN)) if MAX_SEQ_LEN != 256 else \"\"\n","    if CASED: suffix += '_CASED'\n","    if LARGE: suffix += '_LARGE'\n","\n","    TTA_suffixes = [ '',\n","                     '_Alice_Kate_John_Michael',\n","                     '_Elizabeth_Mary_James_Henry',\n","                     '_Kate_Elizabeth_Michael_James',\n","                     '_Mary_Alice_Henry_John']\n","\n","    for TTA_suffix in tqdm(TTA_suffixes):\n","        if is_inference:\n","          \n","          num_test = pd.read_csv(input_tsv,sep='\\t').shape[0]\n","          n_chunk = int(np.ceil(num_test/1000))\n","          for i in range(n_chunk):\n","            print(f\"chunk{i}\")          \n","            extract_data(os.path.basename(input_tsv).split('.')[0] +TTA_suffix+'.tsv', \n","                         os.path.basename(input_tsv).split('.')[0] + '_' + layer+ suffix +TTA_suffix+ f'_{i}_fix_long_text.json',\n","                         start_idx = i*1000, end_idx = min(num_test,(i+1)*1000), is_inference = is_inference)  \n","        else:\n","          extract_data('gap-validation'+TTA_suffix+'.tsv', \n","                       'gap_validation_' + layer+ suffix +TTA_suffix+ '_fix_long_text.json', is_inference = is_inference)\n","\n","          extract_data('gap-test'+TTA_suffix+'.tsv', \n","                       'gap_test_' + layer+ suffix +TTA_suffix+ '_1_fix_long_text.json',\n","                       start_idx = 0, end_idx = 1000, is_inference = is_inference)\n","          extract_data('gap-test'+TTA_suffix+'.tsv', \n","                       'gap_test_' + layer+ suffix +TTA_suffix+ '_2_fix_long_text.json',\n","                       start_idx = 1000, end_idx = 2000, is_inference = is_inference)\n","\n","          extract_data('gap-development'+TTA_suffix+'.tsv', \n","                       'gap_development_' + layer+ suffix+TTA_suffix + '_1_fix_long_text.json',\n","                       start_idx = 0, end_idx = 1000, is_inference = is_inference)\n","          extract_data('gap-development'+TTA_suffix+'.tsv', \n","                       'gap_development_' + layer+ suffix+TTA_suffix + '_2_fix_long_text.json',\n","                       start_idx = 1000, end_idx = 2000, is_inference = is_inference)      \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7QrI-Q7IhpvP","colab_type":"text"},"cell_type":"markdown","source":["## 3. calculate distance features and save as csv files"]},{"metadata":{"id":"04ewyUB3v2eS","colab_type":"text"},"cell_type":"markdown","source":["this section is based on https://www.kaggle.com/chanhu/bert-score-layer-lb-0-475 but it's originally from https://www.kaggle.com/keyit92/coref-by-mlp-cnn-coattention"]},{"metadata":{"id":"zcKD8e3Fd5WQ","colab_type":"code","colab":{}},"cell_type":"code","source":["def bs(lens, target):\n","    low, high = 0, len(lens) - 1\n","\n","    while low < high:\n","        mid = low + int((high - low) / 2)\n","\n","        if target > lens[mid]:\n","            low = mid + 1\n","        elif target < lens[mid]:\n","            high = mid\n","        else:\n","            return mid + 1\n","\n","    return low\n","\n","def bin_distance(dist):\n","    \n","    buckets = [1, 2, 3, 4, 5, 8, 16, 32, 64]  \n","    low, high = 0, len(buckets)\n","    while low < high:\n","        mid = low + int((high-low) / 2)\n","        if dist > buckets[mid]:\n","            low = mid + 1\n","        elif dist < buckets[mid]:\n","            high = mid\n","        else:\n","            return mid\n","\n","    return low\n","\n","def distance_features(P, A, B, char_offsetP, char_offsetA, char_offsetB, text, URL):\n","    \n","    doc = nlp(text)\n","    \n","    lens = [token.idx for token in doc]\n","    mention_offsetP = bs(lens, char_offsetP) - 1\n","    mention_offsetA = bs(lens, char_offsetA) - 1\n","    mention_offsetB = bs(lens, char_offsetB) - 1\n","    \n","    mention_distA = mention_offsetP - mention_offsetA \n","    mention_distB = mention_offsetP - mention_offsetB\n","    \n","    splited_A = A.split()[0].replace(\"*\", \"\")\n","    splited_B = B.split()[0].replace(\"*\", \"\")\n","    \n","    if re.search(splited_A[0], str(URL)):\n","        contains = 0\n","    elif re.search(splited_B[0], str(URL)):\n","        contains = 1\n","    else:\n","        contains = 2\n","    \n","    dist_binA = bin_distance(mention_distA)\n","    dist_binB = bin_distance(mention_distB)\n","    output =  [dist_binA, dist_binB, contains]\n","    \n","    return output\n","\n","def extract_dist_features(df):\n","    \n","    index = df.index\n","    columns = [\"D_PA\", \"D_PB\", \"IN_URL\"]\n","    dist_df = pd.DataFrame(index = index, columns = columns)\n","\n","    for i in tqdm(range(len(df))):\n","        \n","        text = df.loc[i, 'Text']\n","        P_offset = df.loc[i,'Pronoun-offset']\n","        A_offset = df.loc[i, 'A-offset']\n","        B_offset = df.loc[i, 'B-offset']\n","        P, A, B  = df.loc[i,'Pronoun'], df.loc[i, 'A'], df.loc[i, 'B']\n","        URL = df.loc[i, 'URL']\n","        \n","        dist_df.iloc[i] = distance_features(P, A, B, P_offset, A_offset, B_offset, text, URL)\n","        \n","    return dist_df"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d1J1qSn1iA6s","colab_type":"code","outputId":"76ced7a3-3e9a-4f71-a570-82a2e7168884","executionInfo":{"status":"ok","timestamp":1555469358189,"user_tz":240,"elapsed":400061,"user":{"displayName":"Bo Liu","photoUrl":"https://lh5.googleusercontent.com/-yAtjaCoYe44/AAAAAAAAAAI/AAAAAAAAAE8/Jc2yMe60vOc/s64/photo.jpg","userId":"00951788254292121969"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"cell_type":"code","source":["%%time\n","\n","if is_inference:\n","  stage2_df = pd.read_csv(input_tsv,sep='\\t')\n","  stage2_dist_df = extract_dist_features(stage2_df)\n","  out_csv_path = path+'output/stage2_dist_df.csv'\n","  if os.path.exists(out_csv_path): os.remove(out_csv_path)\n","  stage2_dist_df.to_csv(out_csv_path, index=False)\n","  \n","else:\n","  dev_df  = pd.read_csv(path+'input/gap-development.tsv',sep='\\t')\n","  test_df = pd.read_csv(path+'input/gap-test.tsv',sep='\\t')\n","  val_df  = pd.read_csv(path+'input/gap-validation.tsv',sep='\\t')\n","\n","  dev_dist_df  = extract_dist_features(dev_df)\n","  test_dist_df = extract_dist_features(test_df)\n","  val_dist_df  = extract_dist_features(val_df)\n","\n","  dev_dist_df.to_csv(path+'output/dev_dist_df.csv', index=False)\n","  test_dist_df.to_csv(path+'output/test_dist_df.csv', index=False)\n","  val_dist_df.to_csv(path+'output/val_dist_df.csv', index=False)  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 12359/12359 [06:39<00:00, 30.95it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["CPU times: user 9min 7s, sys: 3min 37s, total: 12min 45s\n","Wall time: 6min 39s\n"],"name":"stdout"}]},{"metadata":{"id":"1FIU0ThexjnG","colab_type":"text"},"cell_type":"markdown","source":["## 4. calculate 10 linguistic features and save as csv files"]},{"metadata":{"id":"F125ptbfwEnN","colab_type":"text"},"cell_type":"markdown","source":["this section is based on https://www.kaggle.com/pheell/look-ma-no-embeddings"]},{"metadata":{"trusted":true,"id":"GAeL65zW13Ml","colab_type":"code","colab":{}},"cell_type":"code","source":["# Two useful syntactic relations\n","\n","def domain(t):\n","    while not t._.subj and not t._.poss and\\\n","            not (t.dep_ == 'xcomp' and t.head._.obj) and\\\n","            t != t.head:\n","        t = t.head\n","    return t\n","\n","def ccom(t):\n","    return [t2 for t2 in t.head._.d]\n","\n","spacy.tokens.doc.Doc.set_extension(\n","    'to', method=lambda doc, offset: [t for t in doc if t.idx == offset][0], force=True)\n","spacy.tokens.token.Token.set_extension(\n","    'c', getter=lambda t: [c for c in t.children], force=True)\n","spacy.tokens.token.Token.set_extension(\n","    'd', getter=lambda t: [c for c in t.sent if t in list(c.ancestors)], force=True)\n","spacy.tokens.token.Token.set_extension(\n","    'subj', getter=lambda t: ([c for c in t._.c if c.dep_.startswith('nsubj')] + [False])[0], force=True)\n","spacy.tokens.token.Token.set_extension(\n","    'obj', getter=lambda t: ([c for c in t._.c if c.dep_.startswith('dobj')] + [False])[0], force=True)\n","spacy.tokens.token.Token.set_extension(\n","    'poss', getter=lambda t: ([c for c in t._.c if c.dep_.startswith('poss')] + [False])[0], force=True)\n","spacy.tokens.token.Token.set_extension(\n","    'span', method=lambda t, t2: t.doc[t.i:t2.i] if t.i < t2.i else t.doc[t2.i:t.i], force=True)\n","spacy.tokens.token.Token.set_extension('domain', getter=domain, force=True)\n","spacy.tokens.token.Token.set_extension('ccom', getter=ccom, force=True)"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"LJfogAkq13Mo","colab_type":"code","colab":{}},"cell_type":"code","source":["# Disqualification functions\n","\n","# Prune candidate list given a disqualifying condition (a set of tokens)\n","def applyDisq(condition, candidates, candidate_dict, debug = False):\n","    badnames = sum([nameset(c, candidate_dict) for c in candidates if c in condition[0]], [])\n","    badcands = [c for c in candidates if c.text in badnames]\n","    if debug and len(badcands) > 0: print('Disqualified:', badcands, '<', condition[1])\n","    return [c for c in candidates if c not in badcands]\n","\n","# Apply a list of disqualifying conditions\n","def applyDisqs(conditions, candidates, candidate_dict, debug = False):\n","    for condition in conditions:\n","        if len(candidates) < 1: return candidates\n","        candidates = applyDisq(condition, candidates, candidate_dict, debug)\n","    return candidates\n","\n","# Pass the list of disqualifying conditions for possessive pronouns (his, her)\n","def disqGen(t, candidates, candidate_dict, debug = False):\n","    conds = [(t._.ccom,\n","             \"disqualify candidates c-commanded by genpn; e.g. e.g. *Julia read his_i book about John_i's life.\"),\n","             ([t2 for t2 in candidates if t in t2._.ccom and t2.head.dep_ == 'appos'],\n","             \"disqualify candidates modified by an appositive with genpn; e.g. *I wanted to see John_i, his_i father.\")\n","            ]\n","    return applyDisqs(conds, candidates, candidate_dict, debug)\n","\n","# Pass the list of list of disqualifying conditions for other pronouns\n","def disqOthers(t, candidates, candidate_dict, debug = False):\n","    conds = [([t2 for t2 in t._.ccom if t2.i > t.i],\n","             \"disqualify candidates c-commanded by pn, unless they were preposed;\\\n","             e.g. *He_i cried before John_i laughed. vs. Before John_i laughed, he_i cried.\"),\n","             ([t2 for t2 in candidates if t in t2._.ccom and t2._.domain == t._.domain\n","              and not (t.head.text == 'with' and t.head.head.lemma_ == 'take')],\n","             \"disqualify candidates that c-command pn, unless in different domain;\\\n","             e.g. Mary said that *John_i hit him_i. vs. John_i said that Mary hit him_i;\\\n","             random hard-coded exception: `take with'\"),\n","             ([t2 for t2 in candidates if t2._.domain.dep_ == 'xcomp' and t2._.domain.head._.obj and t2 == t2._.domain.head._.obj],\n","             \"for xcomps with subjects parsed as upstairs dobj, disallow coref with that dobj;\\\n","             e.g. *Mary wanted John_i to forgive him_i.\")\n","            ]\n","    return applyDisqs(conds, candidates, candidate_dict, debug)\n","\n","# Decide whether possessive or not and call appropriate function\n","def disq(t, candidates, candidate_dict, debug = False):\n","    func = disqGen if t.dep_ == 'poss' else disqOthers\n","    candidates = func(t, candidates, candidate_dict, debug)\n","    return candidates"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"TS-tfRRC13Mr","colab_type":"code","colab":{}},"cell_type":"code","source":["# Name functions\n","\n","# Find word of interest at provided offset; sometimes parsed words don't align with provided data, so need to look back\n","def find_head(w, wo, doc):\n","    t = False; backtrack = 0\n","    while not t:\n","        try:\n","            t = doc._.to(wo)\n","        except IndexError:\n","            wo -= 1; backtrack += 1\n","    while t.dep_ == 'compound' and t.head.idx >= wo and t.head.idx < len(w) + wo + backtrack: t = t.head\n","    return t\n","\n","# Returns subsequences of a name\n","def subnames(name):\n","    if type(name) != str: name = candidate_dict[name]\n","    parts = name.split(' ')\n","    subnames_ = []\n","    for i in range(len(parts)): \n","        for j in range(i + 1, len(parts) + 1): \n","            sub = ' '.join(parts[i:j])\n","            if len(sub) > 2: subnames_.append(sub)\n","    return subnames_\n","\n","# Returns subsequences of a name unless potentially ambiguous (if another candidate picks out same subsequence)\n","def nameset(name, candidate_dict):\n","    if type(name) != str: name = candidate_dict[name]\n","    subnames_ = [sn for sn in subnames(name)]\n","    return [c for c in subnames_ if c not in sum([subnames(c) for c in candidate_dict.values() \n","                                                  if c not in subnames_ and name not in subnames(c)], [])]\n","\n","# Given the original candidate dict and the final candidate list, returns new dict grouping putative candidate instances under a single key\n","def candInstances(candidates, candidate_dict):\n","    candidates_by_name = {}\n","    for c in sorted(candidates, key = lambda c: len(candidate_dict[c]), reverse = True):\n","        name = candidate_dict[c]\n","        for name2 in candidates_by_name.keys():\n","            if name in nameset(name2, candidate_dict): name = name2; break\n","        candidates_by_name[name] = candidates_by_name.get(name, []) + [c]\n","    return candidates_by_name\n","\n","import gender_guesser.detector as gender \n","gd = gender.Detector()\n","\n","# Needed to prune candidate dict-- removes non-provided candidates that don't match in most common gender with pn\n","def filterGender(candidates_by_name, a, b, pn):\n","    badnames = []\n","    gender = 'female' if pn in ['She', 'she', 'her', 'Her'] else 'male'\n","    for name in candidates_by_name.keys():\n","        if a in subnames(name) or b in subnames(name): continue\n","        genderii = gd.get_gender(name.split(' ')[0])\n","        if gender == 'male' and genderii == 'female': badnames += [name]; continue\n","        if gender == 'female' and genderii == 'male': badnames += [name]; continue\n","    for name in badnames: candidates_by_name.pop(name)\n","    return candidates_by_name"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"IjXM3tcA13Mt","colab_type":"code","colab":{}},"cell_type":"code","source":["# Metrics\n","\n","from urllib.parse import unquote\n","import re\n","\n","# Authors' metric 1: Does the Wikipedia url contain the candidate's name?\n","def urlMatch(a, b, url, candidate_dict):\n","    url = re.sub('[^\\x00-\\x7F]', '*', unquote(url.split('/')[-1])).replace('_', ' ').lower()\n","    return {'a_url': (sorted([len(n.split(' ')) for n in nameset(a.lower(), candidate_dict) if n in nameset(url, candidate_dict)], reverse = True) + [0])[0],\n","            'b_url': (sorted([len(n.split(' ')) for n in nameset(b.lower(), candidate_dict) if n in nameset(url, candidate_dict)], reverse = True) + [0])[0]}\n","\n","# Authors' metric 2: When pn is subject or object, does the candidate match?\n","def parallel(t1, t2):\n","    if t1.dep_.startswith('nsubj'): return t2.dep_.startswith('nsubj')\n","    if t1.dep_.startswith('dobj'): return t2.dep_.startswith('dobj')\n","    if t1.dep_.startswith('dative'): return t2.dep_.startswith('dative')\n","    return False\n","\n","# Depth from a node to a parent node\n","def depthTo(t1, t2):\n","    depth = 0\n","    while t1 != t2 and t1 != t1.head:\n","        t1 = t1.head\n","        depth += 1\n","    return depth\n","\n","# Syntactic distance within a single tree\n","def nodeDist(t1, t2):\n","    if t1 == t2: return 0\n","    if t2 in t1._.d: return depthTo(t2, t1)\n","    if t1 in t2._.d: return depthTo(t1, t2)\n","    t = t1\n","    while t1 not in t._.d or t2 not in t._.d and t != t.head: t = t.head\n","    return depthTo(t1, t) + depthTo(t2, t)\n","\n","# Authors' metric 3: Syntactic distance (within or across trees)\n","def synDist(t, pn, doc, debug = False):\n","    doc_sents = list(doc.sents)\n","    sspan = doc_sents.index(pn.sent) - doc_sents.index(t.sent)\n","    if sspan == 0: # same sentence\n","        dist = nodeDist(t, pn)\n","    else: # different sentence\n","        dist = nodeDist(pn, doc_sents[doc_sents.index(pn.sent)].root) + nodeDist(t, doc_sents[doc_sents.index(t.sent)].root) # dist from two roots\n","    if debug: \n","        print('pn dist:', nodeDist(pn, doc_sents[doc_sents.index(pn.sent)].root), '; t dist:',\n","              nodeDist(t, doc_sents[doc_sents.index(t.sent)].root), '; span:', sspan)\n","    sspan = abs(sspan) * 1 if sspan >= 0 else abs(sspan) * 1.3 # less local if not preceding\n","    return dist + sspan# * 0.7\n","\n","# Character distance\n","def charDist(t1, t2):\n","    if t2.idx > t1.idx:\n","        return t2.idx - t1.idx + len(t1.text)\n","    else:\n","        return (t1.idx - t2.idx + len(t2.text)) * 1.3\n","\n","# Theta prominence: assign a 0.1 to 1 score based on dep role of candidate -- strong feature\n","def thetaProminence(t, mult = 1, debug = False):\n","    while t.dep_ == 'compound': t = t.head\n","    if debug: print('t dep_:', t.dep_)\n","    if t.dep_ == 'pobj': mult = 1.3 if t.head.i < t.head.head.i else 1\n","    if t._.domain.dep_ == 'advcl': mult = 1.3 if t.head.i < t._.domain.head.i else 1\n","    if t.dep_.startswith('nsubj'): score = 1\n","    elif t.dep_.startswith('dobj'): score = 0.8\n","    elif t.dep_.startswith('dative'): score = 0.6\n","    elif t.dep_.startswith('pobj'): score = 0.4\n","    elif t.dep_.startswith('poss'): score = 0.3\n","    else: score = 0.1\n","    if debug: print('mult:', mult, '; score:', score)\n","    return min(1, score * mult)\n","\n","# Computes these metrics for each candidate, and returns, for each group of instances (A instances, B instances,\\\n","# other instances), either the sum, or the highest difference from the mean\n","def score(label, candidates_by_name, a_cand, b_cand, func, minsc = None, method = 'sum'):\n","    if method == 'sum':\n","        scores = {name: sum([func(t) for t in tokens]) for name, tokens in candidates_by_name.items()}\n","    elif method == 'meandiff':\n","        mean = np.mean(sum([[func(t) for t in tokens] for tokens in candidates_by_name.values()], []))\n","        scores = {name: mean - min([func(t) for t in tokens]) for name, tokens in candidates_by_name.items()}\n","    sca = scores[a_cand] if a_cand else minsc\n","    scb = scores[b_cand] if b_cand else minsc\n","    screst = [v for n, v in scores.items() if n != a_cand and n != b_cand]\n","    if method == 'sum':\n","        screst = sum(screst) if len(screst) > 0 else minsc\n","    elif method == 'meandiff':\n","        screst = max(screst) if len(screst) > 0 else minsc\n","    return {'a_' + label: sca, 'b_' + label: scb, 'n_' + label: screst}"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"mqTRq-G913Mw","colab_type":"code","colab":{}},"cell_type":"code","source":["from tqdm import tqdm_notebook as tqdm\n","\n","# Load a rowfull of data\n","def load_row(data, i):\n","    return tuple(data.iloc[i])\n","\n","# Row by row, populate features\n","def annotateSet(data, minsc = None, debug = False, inference=False):\n","    \n","    annotated_data = pd.DataFrame() # init placeholder df\n","    row_batch = []\n","\n","    for i in tqdm(range(annotated_data.shape[0], data.shape[0])):\n","\n","        if not inference: id, text, pn, pno, a, ao, ag, b, bo, bg, url = load_row(data, i)        \n","        if inference: id, text, pn, pno, a, ao, b, bo, url = load_row(data, i)  \n","\n","        doc = nlp(text) # parse text into doc\n","        pnt, at, bt = (doc._.to(pno), find_head(a, ao, doc), find_head(b, bo, doc)) # get the tokens that correspond to offsets\n","        candidate_dict = {e.root: re.sub('\\'s$', '', e.text) for e in [e for e in doc.ents if e.root.ent_type_ == 'PERSON']} # first get every PERSON ent as candidate\n","        candidate_dict.update({c.root: re.sub('\\'s$', '', c.text) for c in doc.noun_chunks if c.root.pos_ == 'PROPN' and c.text in sum([subnames(n) for n in candidate_dict.values()], []) and\n","                               c.root not in candidate_dict.keys()}) # get some missed ones by looking at noun chunks with PROPN roots whose text match part of a candidate but are not already in list\n","        candidate_dict.update({t: w for t, w in [(at, a), (bt, b)]}) # add provided cands, overwriting in the process\n","\n","        candidates = disq(pnt, list(candidate_dict.keys()), candidate_dict, debug = False)\n","        candidates_by_name = candInstances(candidates, candidate_dict)\n","        candidates_by_name = filterGender(candidates_by_name, a, b, pn)\n","        a_cand = ([name for name, tokens in candidates_by_name.items() if at in tokens] + [False])[0]\n","        b_cand = ([name for name, tokens in candidates_by_name.items() if bt in tokens] + [False])[0]\n","\n","        # init row dict\n","        if not inference: features = {'id': id, 'label': 0 if ag else 1 if bg else 2}\n","        if inference: features = {'id': id, 'label': 0 }\n","        # eliminated or not\n","        features.update({'a_out': 0 if a_cand else 1, 'b_out': 0 if b_cand else 1})\n","        # url match or not\n","        features.update(urlMatch(a, b, url, candidate_dict))\n","        # c-command or not\n","        features.update({'a_cc': 1 if a_cand and pnt in at._.ccom else 0, 'b_cc': 1 if b_cand and pnt in bt._.ccom else 0})\n","        # parallelism score\n","        features.update(score('par', candidates_by_name, a_cand, b_cand, lambda t: parallel(t, pnt), minsc = minsc))\n","        # theta prominence score\n","        features.update(score('th', candidates_by_name, a_cand, b_cand, thetaProminence, minsc = minsc))\n","        # syntactic distance score\n","        features.update(score('loc', candidates_by_name, a_cand, b_cand, lambda t: synDist(t, pnt, doc), method='meandiff', minsc = minsc))\n","        # number of candidates left\n","        features.update({'n_cands': len(candidates_by_name)})\n","        # char dist\n","        features.update(score('cloc', candidates_by_name, a_cand, b_cand, lambda t: charDist(t, pnt), method='meandiff', minsc = minsc))\n","\n","        row_batch += [features]\n","\n","\n","    # add rows to placeholder df\n","    if annotated_data.shape[0] != data.shape[0]: annotated_data = annotated_data.append(row_batch, ignore_index = True)\n","    \n","    return annotated_data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sXTtl8JgdHnZ","colab_type":"code","colab":{}},"cell_type":"code","source":["## post-process: fill na and standardize\n","\n","def post_process(df):\n","  \n","  cols = ['a_cc', 'a_loc', 'a_out', 'a_th', 'a_url', 'b_cc', 'b_loc', 'b_out', 'b_th', 'b_url']\n","  df = df[cols]\n","\n","  df.a_loc.fillna(-5, inplace=True)\n","  df.b_loc.fillna(-5, inplace=True)\n","\n","  df.a_th.fillna(0, inplace=True)\n","  df.b_th.fillna(0, inplace=True)\n","\n","  df.a_url /=5 \n","  df.b_url /=5 \n","\n","  df.a_th /=5\n","  df.b_th /=5\n","\n","  df.a_loc += 8\n","  df.a_loc /= 25\n","  df.b_loc += 8\n","  df.b_loc /=25\n","    \n","  return df.copy()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NInAenMuP18W","colab_type":"code","colab":{}},"cell_type":"code","source":["# %%time\n","\n","if is_inference:\n","  stage2_df = pd.read_csv(input_tsv,sep='\\t')\n","  \n","  ## fixed an input data error in row 1566 where \"(t)he\" is incorrectly tagged as pronoun\n","  ##   this is discussed and allowed in https://www.kaggle.com/c/gendered-pronoun-resolution/discussion/89830\n","  text = stage2_df.loc[1566,'Text']  \n","  stage2_df.loc[1566,'Text'] = text = text[:313] + ' ' + text[313:]\n","  stage2_df.loc[1566,'Pronoun-offset'] += 1\n","  \n","  stage2_lingui_df = annotateSet(stage2_df, inference=True)\n","  stage2_lingui_df = post_process(stage2_lingui_df)\n","  stage2_lingui_df.to_csv(path+'output/stage2_lingui_df.csv', index=False)\n","  \n","else:\n","  dev_df  = pd.read_csv(path+'input/gap-development.tsv',sep='\\t')\n","  test_df = pd.read_csv(path+'input/gap-test.tsv',sep='\\t')\n","  val_df  = pd.read_csv(path+'input/gap-validation.tsv',sep='\\t')\n","\n","  dev_lingui_df  = annotateSet(dev_df)\n","  test_lingui_df = annotateSet(test_df)\n","  val_lingui_df  = annotateSet(val_df)\n","  \n","  dev_lingui_df  = post_process(dev_lingui_df)\n","  test_lingui_df = post_process(test_lingui_df)\n","  val_lingui_df  = post_process(val_lingui_df)  \n","\n","  dev_lingui_df.to_csv(path+'output/dev_lingui_df.csv', index=False)\n","  test_lingui_df.to_csv(path+'output/test_lingui_df.csv', index=False)\n","  val_lingui_df.to_csv(path+'output/val_lingui_df.csv', index=False) "],"execution_count":0,"outputs":[]},{"metadata":{"id":"rPI8kSxJaJog","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}