{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Step2-end2end_model.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"jqptSekdUXln","colab_type":"text"},"cell_type":"markdown","source":["**end2end model: Using dev, test, val jsons from step1, train end2end model for both sub_A and sub_B and save 250 and 50 weights files respectively **\n","\n","sub_A: using 2000 development + 2000 test + 400 validation to train\n","\n","sub_B: using 2000 test + 454 validation to train\n","\n","** Instruction:**\n","\n","run the notebook 4 times, with `all_train = True` (sub_A) and` False` (sub_B) and `CASED = False` and `True` respectively"]},{"metadata":{"id":"JD8GTddpc-38","colab_type":"text"},"cell_type":"markdown","source":["## set up"]},{"metadata":{"id":"qww4nxqTrZV4","colab_type":"code","colab":{}},"cell_type":"code","source":["path = 'drive/My Drive/pronoun/'"],"execution_count":0,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"id":"FO108906rI8M","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","import zipfile\n","import gc\n","from tqdm import tqdm as tqdm\n","import re\n","from glob import glob\n","\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import log_loss"],"execution_count":0,"outputs":[]},{"metadata":{"_uuid":"e265e7becd34d793d13009f851a4fc7c6f7f95fa","trusted":true,"id":"nPXSW1XBrI8h","colab_type":"code","colab":{}},"cell_type":"code","source":["!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\n","!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\n","!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AdTaIgzEdZ1m","colab_type":"text"},"cell_type":"markdown","source":["## features"]},{"metadata":{"id":"aeJ5TllvdBAX","colab_type":"text"},"cell_type":"markdown","source":["### load previously extraced features"]},{"metadata":{"id":"dHbqmB9A9tgH","colab_type":"code","colab":{}},"cell_type":"code","source":["all_train = True\n","CASED = False"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f_EH8SUcYrVW","colab_type":"code","colab":{}},"cell_type":"code","source":["use_lingui_features = True if all_train else False"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-MyKBDQTdnDC","colab_type":"code","outputId":"0794f574-ce03-4c5f-b818-4b76c4342581","executionInfo":{"status":"ok","timestamp":1555204966996,"user_tz":240,"elapsed":679,"user":{"displayName":"Bo Liu","photoUrl":"https://lh5.googleusercontent.com/-yAtjaCoYe44/AAAAAAAAAAI/AAAAAAAAAE8/Jc2yMe60vOc/s64/photo.jpg","userId":"00951788254292121969"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["# dist features and linguistic features\n","\n","if all_train:  \n","  # leave 54 as sanity check\n","  np.random.seed(15)\n","  sanity_idx = np.random.choice(454,54,replace=False)\n","  val_idx = np.setdiff1d(np.arange(454),sanity_idx)  \n","  \n","\n","if not use_lingui_features:\n","\n","  dev_dist_df = pd.read_csv(path+'output/dev_dist_df.csv')\n","  test_dist_df =pd.read_csv(path+'output/test_dist_df.csv')\n","  val_dist_df = pd.read_csv(path+'output/val_dist_df.csv')\n","\n","  if not all_train:\n","    new_dist_df = pd.concat([test_dist_df, val_dist_df]).reset_index(drop=True).copy()\n","    test_dist_df = dev_dist_df.copy()\n","  if all_train:\n","    new_dist_df = pd.concat([dev_dist_df,\n","                             test_dist_df, \n","                             val_dist_df.iloc[val_idx]]).reset_index(drop=True).copy()\n","\n","  print(new_dist_df.shape, test_dist_df.shape)\n","  \n","if use_lingui_features:\n","  dev_lingui_df = pd.read_csv(path+'output/dev_lingui_df.csv')\n","  test_lingui_df =pd.read_csv(path+'output/test_lingui_df.csv')\n","  val_lingui_df = pd.read_csv(path+'output/val_lingui_df.csv')   \n","  \n","  dev_dist_df = pd.concat([pd.read_csv(path+'output/dev_dist_df.csv')[['D_PA','D_PB']], dev_lingui_df], axis=1)\n","  test_dist_df =pd.concat([pd.read_csv(path+'output/test_dist_df.csv')[['D_PA','D_PB']],test_lingui_df],axis=1)\n","  val_dist_df = pd.concat([pd.read_csv(path+'output/val_dist_df.csv')[['D_PA','D_PB']], val_lingui_df], axis=1)\n","\n","  if not all_train:\n","    new_dist_df  = pd.concat([test_dist_df, val_dist_df]).reset_index(drop=True).copy()\n","    test_dist_df = dev_dist_df.copy()\n","\n","  if all_train:\n","    new_dist_df = pd.concat([dev_dist_df,\n","                             test_dist_df, \n","                             val_dist_df.iloc[val_idx]]).reset_index(drop=True).copy()\n","    test_dist_df = val_dist_df.iloc[sanity_idx].reset_index(drop=True).copy()\n","\n","  gc.collect()\n","  print(new_dist_df.shape, test_dist_df.shape  )  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["(4400, 12) (54, 12)\n"],"name":"stdout"}]},{"metadata":{"id":"9W2AYTwI3Yvq","colab_type":"code","colab":{}},"cell_type":"code","source":["def create_input(embed_df, dist_df):\n","    \n","    assert len(embed_df) == len(dist_df)\n","    all_P, all_A, all_B = [] ,[] ,[]\n","    all_label = []\n","    all_dist_PA, all_dist_PB = [], []    \n","    \n","    for i in range(len(embed_df)):\n","        \n","        all_P.append(embed_df.loc[i, \"emb_P\"])\n","        all_A.append(embed_df.loc[i, \"emb_A\"])\n","        all_B.append(embed_df.loc[i, \"emb_B\"])\n","        all_dist_PA.append(dist_df.loc[i, \"D_PA\"])\n","        all_dist_PB.append(dist_df.loc[i, \"D_PB\"])\n","                \n","        label = embed_df.loc[i, \"label\"]\n","        if label == \"A\": \n","            all_label.append(0)\n","        elif label == \"B\": \n","            all_label.append(1)\n","        else: \n","            all_label.append(2)\n","\n","    result_lst = [np.asarray(all_A), np.asarray(all_B), np.asarray(all_P),\n","                  np.expand_dims(np.asarray(all_dist_PA),axis=1),\n","                  np.expand_dims(np.asarray(all_dist_PB),axis=1)]\n","\n","    if use_lingui_features:\n","      for col in dev_lingui_df.columns.values:\n","        result_lst.append(np.expand_dims(dist_df[col].values,axis=1))\n","            \n","    return result_lst, all_label"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FN7fuIwidNxz","colab_type":"code","outputId":"00404575-4a22-49c4-d56c-d7363de6d5ad","executionInfo":{"status":"ok","timestamp":1555205015865,"user_tz":240,"elapsed":39956,"user":{"displayName":"Bo Liu","photoUrl":"https://lh5.googleusercontent.com/-yAtjaCoYe44/AAAAAAAAAAI/AAAAAAAAAE8/Jc2yMe60vOc/s64/photo.jpg","userId":"00951788254292121969"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["# load previously extracted Bert features, orig and Aug\n","\n","LARGE = True\n","\n","layer = \"-4\"\n","suffix = layer\n","if CASED: suffix += '_CASED'\n","if LARGE: suffix += '_LARGE'\n","\n","json_suffix = '_fix_long_text.json'\n","  \n","TTA_suffixes = [ \\\n","                 '_Alice_Kate_John_Michael',\n","                 '_Elizabeth_Mary_James_Henry',\n","                 '_Kate_Elizabeth_Michael_James',\n","                 '_Mary_Alice_Henry_John']\n","\n","d_X_test = {}     # dict for test features\n","new_emb_df_d = {} # dict for train embeddings (to be converted to features later after removing NA)\n","\n","for TTA_suffix in [''] + TTA_suffixes:\n","  gc.collect()\n","  \n","  df_dev1 = pd.read_json(path+'output/contextual_embeddings_gap_development_'+suffix+TTA_suffix+'_1'+json_suffix).sort_index()\n","  df_dev2 = pd.read_json(path+'output/contextual_embeddings_gap_development_'+suffix+TTA_suffix+'_2'+json_suffix).sort_index()\n","  df_test1= pd.read_json(path+'output/contextual_embeddings_gap_test_'+suffix+TTA_suffix+'_1'+json_suffix).sort_index()\n","  df_test2= pd.read_json(path+'output/contextual_embeddings_gap_test_'+suffix+TTA_suffix+'_2'+json_suffix).sort_index()\n","  df_val =  pd.read_json(path+'output/contextual_embeddings_gap_validation_'+suffix+TTA_suffix+json_suffix).sort_index()  \n","  \n","  if not all_train:       \n","    new_emb_df0 = pd.concat([df_test1.sort_index(),\\\n","                             df_test2.sort_index(),\\\n","                             df_val.sort_index()]).reset_index(drop=True).copy()\n","    test_emb0 = pd.concat([df_dev1.sort_index(),\\\n","                           df_dev2.sort_index()]).reset_index(drop=True).copy()\n","\n","  if all_train:  \n","    # leave 54 as sanity check\n","    np.random.seed(15)\n","    sanity_idx = np.random.choice(454,54,replace=False)\n","    val_idx = np.setdiff1d(np.arange(454),sanity_idx)     \n","    \n","    test_emb0 = df_val.iloc[sanity_idx].reset_index(drop=True).copy()  \n","    new_emb_df0 = pd.concat([df_dev1.sort_index(),\n","                            df_dev2.sort_index(),\n","                            df_test1.sort_index(),\n","                            df_test2.sort_index(),\n","                            df_val.sort_index().iloc[val_idx]]).reset_index(drop=True).copy() \n","\n","  # put into dictionary\n","  key = 'orig' if TTA_suffix=='' else TTA_suffix.strip('_')    \n","\n","  X_test0, y_test = create_input(test_emb0, test_dist_df)    \n","  d_X_test[key] = X_test0.copy()\n","\n","  new_emb_df_d[key] = new_emb_df0.copy()\n","\n","print(d_X_test['orig'][0].shape, len(new_emb_df_d['orig'].iloc[0].emb_A))  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["(54, 1024) 1024\n"],"name":"stdout"}]},{"metadata":{"id":"L1BWPEOeiejQ","colab_type":"text"},"cell_type":"markdown","source":["### fix wrong labels"]},{"metadata":{"id":"1QT5-9ZuDpwC","colab_type":"code","outputId":"8aaa21db-cb1c-4b00-969f-c2dc0328a650","executionInfo":{"status":"ok","timestamp":1555205022125,"user_tz":240,"elapsed":2833,"user":{"displayName":"Bo Liu","photoUrl":"https://lh5.googleusercontent.com/-yAtjaCoYe44/AAAAAAAAAAI/AAAAAAAAAE8/Jc2yMe60vOc/s64/photo.jpg","userId":"00951788254292121969"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["if all_train: \n","  test_val_df = pd.concat([pd.read_table(path+'input/gap-development-corrected-74.tsv')[['ID','A-coref','B-coref']],\n","                           pd.read_table(path+'input/gap-test-val-85.tsv')[['ID','A-coref','B-coref']].iloc[:2000],\n","                           pd.read_table(path+'input/gap-test-val-85.tsv')[['ID','A-coref','B-coref']].iloc[2000:].iloc[val_idx]]).reset_index(drop=True)\n","else:\n","  test_val_df = pd.read_table(path+'input/gap-test-val-85.tsv')[['ID','A-coref','B-coref']].reset_index(drop=True)\n","  \n","  \n","test_val_df['label'] = test_val_df.apply(lambda x:'A' if x['A-coref'] else ('B' if x['B-coref'] else 'Neither'),axis=1)\n","\n","print(\"number of wrong labels = {:d}\".format((test_val_df.label!=new_emb_df_d['orig'].label).sum()))\n","\n","# fix labels\n","new_emb_df_d['orig'].label = test_val_df.label\n","\n","assert (test_val_df.label!=new_emb_df_d['orig'].label).sum() == 0"],"execution_count":0,"outputs":[{"output_type":"stream","text":["number of wrong labels = 156\n"],"name":"stdout"}]},{"metadata":{"id":"V1lGyC-TjYNg","colab_type":"text"},"cell_type":"markdown","source":["### remove None rows"]},{"metadata":{"id":"KiQF1yWEjaym","colab_type":"code","outputId":"e88b6e1c-ba80-4607-cd70-d9e6555492b9","executionInfo":{"status":"ok","timestamp":1555205035912,"user_tz":240,"elapsed":2901,"user":{"displayName":"Bo Liu","photoUrl":"https://lh5.googleusercontent.com/-yAtjaCoYe44/AAAAAAAAAAI/AAAAAAAAAE8/Jc2yMe60vOc/s64/photo.jpg","userId":"00951788254292121969"}},"colab":{"base_uri":"https://localhost:8080/","height":217}},"cell_type":"code","source":["TTA_suffixes = [ 'Alice_Kate_John_Michael',\n","                 'Elizabeth_Mary_James_Henry',\n","                 'Kate_Elizabeth_Michael_James',\n","                 'Mary_Alice_Henry_John']\n","\n","for TTA_suffix in ['orig'] + TTA_suffixes:\n","  bad_rows = []\n","  for i in range(new_emb_df_d[TTA_suffix].shape[0]):\n","    for col in ['emb_A','emb_B','emb_P']:\n","      if None in new_emb_df_d[TTA_suffix].loc[i,col]:\n","        bad_rows.append(i)\n","        break\n","  print(TTA_suffix, 'bad_rows =', bad_rows)  \n","  \n","## remove None\n","\n","# remove bad rows in (common) dist df\n","new_dist_df = new_dist_df.drop(bad_rows).reset_index(drop=True)\n","\n","print(' ')  \n","\n","# and remove them in all emb df\n","for TTA_suffix in ['orig'] + TTA_suffixes:  \n","    \n","  new_emb_df_d[TTA_suffix] = new_emb_df_d[TTA_suffix].drop(bad_rows).reset_index(drop=True)\n","  \n","  assert new_emb_df_d[TTA_suffix].shape[0]==new_dist_df.shape[0]\n","\n","  print(TTA_suffix, new_emb_df_d[TTA_suffix].shape)     "],"execution_count":0,"outputs":[{"output_type":"stream","text":["orig bad_rows = []\n","Alice_Kate_John_Michael bad_rows = []\n","Elizabeth_Mary_James_Henry bad_rows = []\n","Kate_Elizabeth_Michael_James bad_rows = []\n","Mary_Alice_Henry_John bad_rows = []\n"," \n","orig (4400, 4)\n","Alice_Kate_John_Michael (4400, 4)\n","Elizabeth_Mary_James_Henry (4400, 4)\n","Kate_Elizabeth_Michael_James (4400, 4)\n","Mary_Alice_Henry_John (4400, 4)\n"],"name":"stdout"}]},{"metadata":{"id":"ksWxhIvxkNyt","colab_type":"code","outputId":"67b0c102-ef48-4cdd-b12b-2376f6849188","executionInfo":{"status":"ok","timestamp":1555205050944,"user_tz":240,"elapsed":5679,"user":{"displayName":"Bo Liu","photoUrl":"https://lh5.googleusercontent.com/-yAtjaCoYe44/AAAAAAAAAAI/AAAAAAAAAE8/Jc2yMe60vOc/s64/photo.jpg","userId":"00951788254292121969"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["X_train_d = {}\n","for TTA_suffix in TTA_suffixes + ['orig']: \n","  X_train0, y_train = create_input(new_emb_df_d[TTA_suffix], new_dist_df)\n","  X_train_d[TTA_suffix] = X_train0.copy()\n","\n","  \n","y_one_hot = np.zeros((len(y_test), 3))\n","for i in range(len(y_test)): y_one_hot[i, y_test[i]] = 1\n","  \n","y_train_one_hot = np.zeros((len(y_train), 3))\n","for i in range(len(y_train)): y_train_one_hot[i, y_train[i]] = 1\n","  \n","gc.collect()  \n","y_train_one_hot.shape, y_one_hot.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((4400, 3), (54, 3))"]},"metadata":{"tags":[]},"execution_count":13}]},{"metadata":{"id":"68-jWWtcYZkH","colab_type":"text"},"cell_type":"markdown","source":["## keras model"]},{"metadata":{"id":"dcq7aQgIxD9a","colab_type":"text"},"cell_type":"markdown","source":["the model architecture is from https://arxiv.org/abs/1707.07045  \n","\n","the implementation is based on https://www.kaggle.com/mateiionita/taming-the-bert-a-baseline"]},{"metadata":{"id":"e2uBI3XuYS3M","colab_type":"code","outputId":"ddb51115-1909-40be-e20a-b1d78c5677ac","executionInfo":{"status":"ok","timestamp":1555205058684,"user_tz":240,"elapsed":2407,"user":{"displayName":"Bo Liu","photoUrl":"https://lh5.googleusercontent.com/-yAtjaCoYe44/AAAAAAAAAAI/AAAAAAAAAE8/Jc2yMe60vOc/s64/photo.jpg","userId":"00951788254292121969"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["from keras.layers import *\n","import keras.backend as K\n","from keras.models import *\n","import keras\n","from keras import optimizers\n","from keras import callbacks\n","from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot\n","\n","class End2End_NCR():\n","    \n","    def __init__(self, word_input_shape, dist_shape, embed_dim=20): \n","        \n","        self.word_input_shape = word_input_shape\n","        self.dist_shape   = dist_shape\n","        self.embed_dim    = embed_dim\n","        self.buckets      = [1, 2, 3, 4, 5, 8, 16, 32, 64] \n","        self.hidden_dim   = 150\n","        self.dense_layer_sizes = [512,32]\n","        self.dropout_rate = 0.6\n","        \n","    def build(self):\n","        \n","        A, B, P = Input((self.word_input_shape,)), Input((self.word_input_shape,)), Input((self.word_input_shape,))\n","        inputs = [A, B, P]\n","        if use_lingui_features: dist_inputs = [Input((1,)) for i in range(len(X_test0)-3)]\n","        else: dist_inputs = [dist1, dist2]\n","        \n","        self.dist_embed = Embedding(10, self.embed_dim)\n","        self.ffnn       = Sequential([Dense(self.hidden_dim, use_bias=True),\n","                                     Activation('relu'),\n","                                     Dropout(rate=0.2, seed = 7),\n","                                     Dense(1, activation='linear')])              \n","        \n","        dist_embeds = [self.dist_embed(dist) for dist in dist_inputs[:2]]\n","        dist_embeds = [Flatten()(dist_embed) for dist_embed in dist_embeds]\n","        \n","        #Scoring layer\n","        #In https://www.aclweb.org/anthology/D17-1018, \n","        #used feed forward network which measures if it is an entity mention using a score\n","        #because we already know the word is mention.\n","        #In here, I just focus on the pairwise score\n","        PA = Multiply()([inputs[0], inputs[2]])\n","        PB = Multiply()([inputs[1], inputs[2]])\n","        #PairScore: sa(i,j) =wa·FFNNa([gi,gj,gi◦gj,φ(i,j)])\n","        # gi is embedding of Pronoun\n","        # gj is embedding of A or B\n","        # gi◦gj is element-wise multiplication\n","        # φ(i,j) is the distance embedding\n","        if use_lingui_features:\n","          PA = Concatenate(axis=-1)([P, A, PA, dist_embeds[0]] + [dist_inputs[i] for i in [2,3,4,5,6]])\n","          PB = Concatenate(axis=-1)([P, B, PB, dist_embeds[1]] + [dist_inputs[i] for i in [7,8,9,10,11]])\n","        else:\n","          PA = Concatenate(axis=-1)([P, A, PA, dist_embeds[0]])\n","          PB = Concatenate(axis=-1)([P, B, PB, dist_embeds[1]])\n","        PA_score = self.ffnn(PA)\n","        PB_score = self.ffnn(PB)\n","        # Fix the Neither to score 0.\n","        score_e  = Lambda(lambda x: K.zeros_like(x))(PB_score)\n","        \n","        #Final Output\n","        output = Concatenate(axis=-1)([PA_score, PB_score, score_e]) # [Pronoun and A score, Pronoun and B score, Neither Score]\n","        output = Activation('softmax')(output)        \n","        model = Model(inputs+dist_inputs, output)\n","        \n","        return model"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"q0Y-OlgYYccy","colab_type":"code","outputId":"5ae38e90-c8b7-46a6-e3ea-0b04c89a2729","executionInfo":{"status":"ok","timestamp":1555205062487,"user_tz":240,"elapsed":538,"user":{"displayName":"Bo Liu","photoUrl":"https://lh5.googleusercontent.com/-yAtjaCoYe44/AAAAAAAAAAI/AAAAAAAAAE8/Jc2yMe60vOc/s64/photo.jpg","userId":"00951788254292121969"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"cell_type":"code","source":["batch_size = 128\n","lr = 3e-5 if all_train else 1e-4\n","patience_orig = 100 \n","patience = patience_orig\n","epochs = 1000\n","n_fold = 5\n","\n","suffix = layer\n","if CASED: suffix += '_CASED'\n","if LARGE: suffix += '_LARGE'\n","  \n","  \n","TTA_suffixes = \\\n","['orig',\n"," 'Alice_Kate_John_Michael',\n"," 'Elizabeth_Mary_James_Henry',\n"," 'Kate_Elizabeth_Michael_James',\n"," 'Mary_Alice_Henry_John']\n","\n","Aug_suffixes = \\\n","['Alice_Kate_John_Michael',\n"," 'Elizabeth_Mary_James_Henry',\n"," 'Kate_Elizabeth_Michael_James',\n"," 'Mary_Alice_Henry_John']  \n","\n","if len(Aug_suffixes)==3: suffix += '_Aug3'\n","if len(Aug_suffixes)==4: suffix += '_Aug4'  \n","suffix += '_all_train' if all_train else '_sub_B'\n","  \n","suffix += '_4400'  \n","suffix += '_Lingui_10'\n","  \n","print(suffix)\n","\n","num_test = len(y_test)\n","num_train = len(y_train)\n","print(num_train,num_test)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["-4_LARGE_Aug4_all_train_4400_Lingui_10\n","4400 54\n"],"name":"stdout"}]},{"metadata":{"id":"ZrI8Cnrrbc6G","colab_type":"code","colab":{}},"cell_type":"code","source":["#####  all_train\n","# pred_oof, sub_all_d: OOF\n","# pred_all_d, sub_df : sanity\n","\n","#####  sub_B\n","# pred_all_d, sub_df : dev\n","\n","%%time\n","\n","if all_train:\n","\n","  for run in range(0,5):  \n","    gc.collect()\n","\n","    if all_train:\n","      sub_all = pd.concat([pd.read_table(path+'input/gap-development.tsv',usecols=['ID']),\n","                           pd.read_table(path+'input/gap-test.tsv',usecols=['ID']),\n","                           pd.read_table(path+'input/gap-validation.tsv',usecols=['ID']).iloc[val_idx]]).\\\n","                reset_index(drop=True).drop(bad_rows)\n","      sub_all['A']=0; sub_all['B']=0; sub_all['NEITHER']=0\n","      sub_all_d = {}\n","      for TTA_suffix in TTA_suffixes: sub_all_d[TTA_suffix] = sub_all.copy()  \n","\n","    pred_all_d = {} # to save 25 fold avg (for Test), 5 outer OOF, 5 inner early stop\n","    for TTA_suffix in TTA_suffixes: pred_all_d[TTA_suffix] = np.zeros((num_test,3))        \n","\n","    # outer 5 fold: OOF fold. 4/5 train, 1/5 OOF pred  \n","    kfold = KFold(n_splits=n_fold, shuffle=True, random_state=3)\n","    for fold_n, (train_fold_index, oof_val_index) in enumerate(kfold.split(X_train_d['orig'][0])):\n","        y_train_fold  = np.asarray(y_train)[train_fold_index]\n","        y_oof_val = np.asarray(y_train)[oof_val_index]      \n","\n","        X_train_fold_d = {}; X_oof_val_d = {}\n","        for TTA_suffix in TTA_suffixes: \n","          X_train_fold_d[TTA_suffix] = [inputs[train_fold_index] for inputs in X_train_d[TTA_suffix]]    \n","          X_oof_val_d[TTA_suffix] = [inputs[oof_val_index] for inputs in X_train_d[TTA_suffix]]    \n","\n","        # inner 5 fold: train and early-stop val fold.\n","        kfold_inner = KFold(n_splits=n_fold, shuffle=True, random_state=5)\n","        for fold_n_inner, (train_index, valid_index) in enumerate(kfold_inner.split(X_train_fold_d['orig'][0])):\n","\n","            X_tr  = [inputs[train_index] for inputs in X_train_fold_d['orig']] \n","            X_tr_orig = X_tr.copy()\n","            X_val = [inputs[valid_index] for inputs in X_train_fold_d['orig']]   \n","            y_tr  = np.asarray(y_train_fold)[train_index]\n","            y_val = np.asarray(y_train_fold)[valid_index]\n","\n","            # train augmentation\n","            if len(Aug_suffixes)>1: \n","              patience = np.ceil(patience_orig / (1+len(Aug_suffixes)))\n","              for k in Aug_suffixes:\n","                X_tr0 = [inputs[train_index] for inputs in X_train_fold_d[k]]\n","\n","                for i in range(len(X_tr)): X_tr[i] = np.concatenate((X_tr[i], X_tr0[i]),axis=0)\n","                y_tr = np.concatenate((y_tr, y_tr),axis=0)\n","\n","              arr = np.arange(X_tr[0].shape[0])\n","              np.random.shuffle(arr)\n","              X_tr = [X_tr0[arr,:] for X_tr0 in X_tr]\n","              y_tr = y_tr[arr]      \n","              print(X_tr[0].shape, y_tr.shape, X_val[0].shape, y_val.shape)  \n","\n","            model = End2End_NCR(word_input_shape=X_train_fold_d['orig'][0].shape[1], dist_shape=X_train_fold_d['orig'][3].shape[1]).build()\n","            model.compile(optimizer=optimizers.Adam(lr=lr), loss=\"sparse_categorical_crossentropy\")\n","            file_path = path + 'wts/e2e' + suffix + \"_{}{}{}.hdf5\".format(run,fold_n,fold_n_inner)\n","            check_point = callbacks.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 0, save_best_only = True, mode = \"min\")\n","            early_stop = callbacks.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=patience, restore_best_weights = True)    \n","            hist = model.fit(X_tr, y_tr, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=0,\n","                      shuffle=True, callbacks = [check_point, early_stop])\n","\n","            for TTA_suffix in TTA_suffixes:\n","              pred = model.predict(x = d_X_test[TTA_suffix], verbose = 0)\n","              pred_all_d[TTA_suffix] += pred / n_fold / n_fold \n","              if all_train:\n","                pred_oof = model.predict(x = X_oof_val_d[TTA_suffix], verbose=0)      \n","                sub_all_d[TTA_suffix].loc[sub_all_d[TTA_suffix].index[oof_val_index],['A','B','NEITHER']] += pred_oof / n_fold\n","\n","\n","    for TTA_suffix in TTA_suffixes:    \n","      # for Test\n","      if all_train:\n","        sub_df = pd.read_csv(path+'input/gap-validation.tsv',sep='\\t').iloc[sanity_idx][['ID']]\n","        sub_df['A'] = 1/3; sub_df['B'] = 1/3; sub_df['NEITHER'] = 1/3 \n","      else:\n","        sub_df = pd.read_csv(path+'input/sample_submission_stage_1.csv')\n","\n","      sub_df.loc[:,['A','B','NEITHER']] = pred_all_d[TTA_suffix]      \n","      sub_df.to_csv(path+'sub/end2end'+suffix+'_'+TTA_suffix+'_run{:d}_{:.5f}.csv'.format(run,log_loss(y_one_hot, pred_all_d[TTA_suffix])), index=False)        \n","      print(f'run{run} {TTA_suffix} ' + \"{:d}folds {:.5f}\".format(n_fold, log_loss(y_one_hot, pred_all_d[TTA_suffix]))) # Calculate the log loss \n","\n","      if all_train:\n","        sub_all_d[TTA_suffix].to_csv(path+'sub/oof'+suffix+'_'+TTA_suffix+'_run{:d}_{:.5f}.csv'.format(run,log_loss(y_train_one_hot, sub_all_d[TTA_suffix].loc[:,['A','B','NEITHER']].values)), index=False)\n","        print(f'run{run} {TTA_suffix} ' + \"{:d}folds OOF ================= {:.5f}\".format(n_fold, log_loss(y_train_one_hot, sub_all_d[TTA_suffix].loc[:,['A','B','NEITHER']]))) # Calculate the log loss    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"fQA0Hfe9cbMK","colab_type":"code","colab":{}},"cell_type":"code","source":["#####  all_train\n","# pred_oof, sub_all_d: OOF\n","# pred_all_d, sub_df : sanity\n","\n","#####  sub_B\n","# pred_all_d, sub_df : dev\n","\n","%%time\n","\n","if not all_train:\n","\n","  for run in range(0,5):  \n","    gc.collect()\n","\n","    pred_all_d = {} \n","    for TTA_suffix in TTA_suffixes: pred_all_d[TTA_suffix] = np.zeros((num_test,3))        \n","\n","    kfold = KFold(n_splits=n_fold, shuffle=True, random_state=3)\n","    for fold_n, (train_index, valid_index) in enumerate(kfold.split(X_train_d['orig'][0])):   \n","\n","        X_tr  = [inputs[train_index] for inputs in X_train_d['orig']] \n","        X_tr_orig = X_tr.copy()\n","        X_val = [inputs[valid_index] for inputs in X_train_d['orig']]   \n","        y_tr  = np.asarray(y_train)[train_index]\n","        y_val = np.asarray(y_train)[valid_index]\n","\n","        # train augmentation\n","        if len(Aug_suffixes)>1: \n","          patience = np.ceil(patience_orig / (1+len(Aug_suffixes)))\n","          for k in Aug_suffixes:\n","            X_tr0 = [inputs[train_index] for inputs in X_train_d[k]]\n","\n","            for i in range(len(X_tr)): X_tr[i] = np.concatenate((X_tr[i], X_tr0[i]),axis=0)\n","            y_tr = np.concatenate((y_tr, y_tr),axis=0)\n","\n","          arr = np.arange(X_tr[0].shape[0])\n","          np.random.shuffle(arr)\n","          X_tr = [X_tr0[arr,:] for X_tr0 in X_tr]\n","          y_tr = y_tr[arr]      \n","          print(X_tr[0].shape, y_tr.shape, X_val[0].shape, y_val.shape)  \n","\n","        model = End2End_NCR(word_input_shape=X_train_d['orig'][0].shape[1], dist_shape=X_train_d['orig'][3].shape[1]).build()\n","        model.compile(optimizer=optimizers.Adam(lr=lr), loss=\"sparse_categorical_crossentropy\")\n","        file_path = path + 'wts/e2e' + suffix + \"_{}{}.hdf5\".format(run,fold_n)\n","        check_point = callbacks.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 0, save_best_only = True, mode = \"min\")\n","        early_stop = callbacks.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=patience, restore_best_weights = True)    \n","        hist = model.fit(X_tr, y_tr, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=0,\n","                  shuffle=True, callbacks = [check_point, early_stop])\n","\n","        for TTA_suffix in TTA_suffixes:\n","          pred = model.predict(x = d_X_test[TTA_suffix], verbose = 0)\n","          pred_all_d[TTA_suffix] += pred / n_fold  \n","\n","    for TTA_suffix in TTA_suffixes:    \n","      sub_df = pd.read_csv(path+'input/sample_submission_stage_1.csv')\n","\n","      sub_df.loc[:,['A','B','NEITHER']] = pred_all_d[TTA_suffix]      \n","      sub_df.to_csv(path+'sub/end2end'+suffix+'_'+TTA_suffix+'_run{:d}_{:.5f}.csv'.format(run,log_loss(y_one_hot, pred_all_d[TTA_suffix])), index=False)        \n","      print(f'run{run} {TTA_suffix} ' + \"{:d}folds {:.5f}\".format(n_fold, log_loss(y_one_hot, pred_all_d[TTA_suffix]))) # Calculate the log loss \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Grco96FofdJt","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}